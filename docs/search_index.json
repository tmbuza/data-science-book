[["index.html", "Learn, Compare, and Master Data Science in Python &amp; R Preface Motive", " Learn, Compare, and Master Data Science in Python &amp; R Preface This bilingual guide is designed to help beginners learn data science using both Python and R. The content is presented side by side in both languages, providing a clear comparison and making it easier to understand key data science concepts. Covering essential topics such as data loading, cleaning, visualization, and basic machine learning techniques, this step-by-step guide ensures that learners build strong foundational skills. The book follows a Q&amp;A format, making it accessible to both students and instructors. Each chapter is structured to facilitate easy navigation, and chapters are displayed in the left navigation bar. Simply click the hyperlinked text to jump to specific chapters or sections. Motive In recent years, artificial intelligence (AI) has made remarkable strides, with tools like ChatGPT and automated coding assistants becoming widely available. These technologies can accelerate coding, automate data analysis, and even generate complex models. However, a well-structured, human-curated approach remains essential for ensuring accuracy, interpretability, and ethical considerations in data science. Python and R are two of the most popular programming languages in data science, each offering unique strengths. Python is known for its simplicity, versatility, and powerful libraries like pandas, numpy, matplotlib, and scikit-learn, which make data manipulation, visualization, and machine learning accessible. It integrates well with various technologies, making it ideal for both beginners and experienced data scientists. R, designed specifically for data analysis and visualization, excels at statistical modeling and high-quality graphical representations with libraries like ggplot2, dplyr, and caret. It is widely used in academic research and industries that require robust statistical analysis. This guide presents Python and R solutions side by side, allowing learners to appreciate the strengths of each language. The content begins with the fundamental tasks needed to get started, such as setting up your development environment, organizing your project directory, and loading your first dataset. In the beginner version, these initial steps will help you quickly understand how to use VSCode, organize your project, and take your first steps into data science with Python and R. By working through this guide, learners will build a solid foundation in data science, preparing them for real-world applications. AI-generated insights should always be complemented by human expertise in interpreting, refining, and validating results. Updated on Mar 18, 2025 "],["setup-dev-envt.html", "1 Setting Up Your Development Environment 1.1 Install Python 1.2 Install R 1.3 Install VSCode", " 1 Setting Up Your Development Environment Before diving into the Q&amp;A sections, it’s important to ensure your environment is properly set up. This section will guide you through the steps to prepare everything you need to start working with both Python and R. To begin working with data science, you’ll need to set up your development environment. Follow the steps below to install and configure the necessary tools. 1.1 Install Python Visit the official Python website to download and install the latest version of Python. During installation, ensure that you check the box to add Python to your PATH. 1.2 Install R Visit the official R Project website to download and install the latest version of R. Follow the installation instructions for your operating system. 1.3 Install VSCode 1.3.1 Why Use VSCode? Visual Studio Code (VSCode) is a lightweight, free, and powerful code editor that supports multiple programming languages, including Python and R. It provides an interactive environment for writing, running, and debugging code efficiently. 1.3.2 Installation Steps: Download and install Visual Studio Code (VSCode) from the official VSCode website. Once installed, open VSCode and install the necessary extensions to enable support for Python and R. 1.3.3 Installing Extensions in VSCode Extensions enhance the functionality of VSCode by adding language support, debugging tools, and other useful features. For Python: Open VSCode. Press Ctrl + Shift + X (Windows/Linux) or Cmd + Shift + X (Mac) to open the Extensions Marketplace. Search for Python (developed by Microsoft). Click Install. This extension provides: IntelliSense (code completion, function suggestions, and real-time error checking). Debugging support. Jupyter Notebook integration. Formatting and linting tools. For R: In the Extensions Marketplace, search for R. Click Install. This extension provides: IntelliSense for R functions and datasets. Integrated R terminal for running scripts. Debugging support. What is IntelliSense? IntelliSense is an advanced code assistance feature that helps you write code faster and with fewer errors by providing autocomplete suggestions, function hints, and real-time error checking. It makes coding in Python and R easier by suggesting function names, displaying expected arguments, and highlighting errors before you run the code. Verify Installation After installing VSCode and the extensions: Open VSCode and check if the extensions are enabled. Open a Python or R script to ensure syntax highlighting and IntelliSense work correctly. If using R, ensure you have R installed on your system so that the R extension can detect it. Now, you’re ready to start coding in both Python and R using VSCode! 🚀 "],["how-to-install-basic-libaries-for-python-and-r.html", "2 How to Install Basic Libaries for Python and R 2.1 Explanation 2.2 Python Code 2.3 R Code", " 2 How to Install Basic Libaries for Python and R 2.1 Explanation Before you can load datasets in Python and R, you need to install the necessary libraries. Here’s how you can install the basic libraries required for this guide: 2.2 Python Code In your terminal, run the following command to install the necessary libraries for data manipulation, visualization, and machine learning: pip install pandas matplotlib scikit-learn 2.3 R Code In R, install the following packages to help with data manipulation and visualization: if (!require(tidyverse)) {install.packages(&#39;tidyverse&#39;)} library(tidyverse) if (!require(caret)) {install.packages(&#39;caret&#39;)} library(caret) The tidyverse R package contains a collection of essential packages for data science, including: ggplot2 for data visualization dplyr for data manipulation tidyr for data tidying readr for reading and writing data files purrr for functional programming tibble for an improved data frame format stringr for string manipulation forcats for categorical variable handling The caret package (short for Classification and Regression Training) is widely used for machine learning in R. It provides a unified interface for training and evaluating models, making it easier to apply machine learning techniques. These libraries and packages will ensure you have the tools you need to get started with data analysis and visualization in both languages. "],["set-project-dir.html", "3 How to Create a Project Directory in Python and R 3.1 Explanation 3.2 Python Code 3.3 R Code", " 3 How to Create a Project Directory in Python and R 3.1 Explanation A well-organized project directory is key to efficient data science work. In this section, you will set up a project directory with separate folders for your datasets and scripts. Now that you have your environment set up, it’s time to create your project directory. This will help keep your files organized as you progress through the guide. Create a new folder for your project, such as beginner-data-science. Inside this folder, create the following subfolders: data: This folder will store your datasets. scripts: Store Python or R scripts here. images: Use this folder for images related to the project. Example Structure: beginner-data-science/ ├── data/ ├── scripts/ └── images/ 3.2 Python Code import os # Define project structure project_dir = &#39;./&#39; data_dir = os.path.join(project_dir, &#39;data&#39;) scripts_dir = os.path.join(project_dir, &#39;scripts&#39;) images_dir = os.path.join(project_dir, &#39;images&#39;) # Create directories os.makedirs(data_dir, exist_ok=True) os.makedirs(scripts_dir, exist_ok=True) os.makedirs(images_dir, exist_ok=True) print(f&quot;Project directory structure created at {project_dir}&quot;) Project directory structure created at ./ 3.3 R Code # Define project structure project_dir &lt;- &quot;./&quot; data_dir &lt;- file.path(project_dir, &quot;data&quot;) scripts_dir &lt;- file.path(project_dir, &quot;scripts&quot;) images_dir &lt;- file.path(project_dir, &quot;images&quot;) # Create directories dir.create(data_dir, showWarnings = FALSE) dir.create(scripts_dir, showWarnings = FALSE) dir.create(images_dir, showWarnings = FALSE) cat(&quot;Project directory structure created at&quot;, project_dir, &quot;\\n&quot;) Project directory structure created at ./ "],["how-to-save-a-dataset-in-python-and-r.html", "4 How to Save a Dataset in Python and R 4.1 Explanation 4.2 Python Code 4.3 R Code", " 4 How to Save a Dataset in Python and R 4.1 Explanation Saving datasets is essential for storing processed data, sharing results, and reusing data in later analysis. In Python, we commonly use pandas to save datasets in CSV format. In R, readr::write_csv() and write.csv() are common functions for saving datasets. You will need to: Iris dataset is available from this link. Also comes with some python libraries such as sklearn.datasets in python and data() in R. Note! When working with datasets in both Python and R, it’s essential to save them in a structured format. However, the column names in the Iris dataset differ slightly between Python and R: Python (pandas version) uses: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), species R (datasets::iris version) uses: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species To maintain consistency, we will save two separate versions: iris-py.csv (from Python) iris-r.csv (from R) This ensures that each dataset retains its original structure before standardization. 4.2 Python Code import pandas as pd from sklearn.datasets import load_iris # Load the full iris dataset iris = load_iris(as_frame=True) df = iris.frame # Add species names df[&quot;species&quot;] = df[&quot;target&quot;].map({0: &quot;setosa&quot;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) # Save the dataset as &quot;iris-py.csv&quot; df.to_csv(&quot;data/iris_py.csv&quot;, index=False) print(&quot;Dataset saved as &#39;data/iris_py.csv&#39;&quot;) Dataset saved as &#39;data/iris_py.csv&#39; # Add species names df[&quot;species&quot;] = df[&quot;target&quot;].map({0: &quot;setosa&quot;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) # Save dataset with all columns df.to_csv(&quot;data/iris_py.csv&quot;, index=False) print(&quot;Dataset saved as &#39;data/iris_py.csv&#39;&quot;) Dataset saved as &#39;data/iris_py.csv&#39; 4.3 R Code # Load necessary libraries library(readr) # Load the full iris dataset df &lt;- datasets::iris # Save the dataset with species included using write_csv from readr write_csv(df, &quot;data/iris_r.csv&quot;) # Confirmation message print(&quot;Dataset saved as &#39;data/iris_r.csv&#39;&quot;) [1] &quot;Dataset saved as &#39;data/iris_r.csv&#39;&quot; "],["how-to-rename-column-names-in-python-and-r.html", "5 How to Rename Column Names in Python and R? 5.1 Explanation 5.2 Python Code 5.3 R Code", " 5 How to Rename Column Names in Python and R? 5.1 Explanation Since the column names differ between Python and R versions of the dataset, we will standardize them to ensure consistency. The renamed column names will be: sepal_length sepal_width petal_length petal_width species This makes it easier to work with the dataset across different tools and languages. After Renaming, we will save the final dataset as iris.csv. 5.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris_py.csv&quot;) # Rename columns df.rename(columns={&#39;sepal length (cm)&#39;: &#39;sepal_length&#39;, &#39;sepal width (cm)&#39;: &#39;sepal_width&#39;, &#39;petal length (cm)&#39;: &#39;petal_length&#39;, &#39;petal width (cm)&#39;: &#39;petal_width&#39;, &#39;species&#39;: &#39;species&#39;}, inplace=True) # Save the standardized dataset df.to_csv(&quot;data/iris.csv&quot;, index=False) print(&quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot;) Renamed dataset saved as &#39;data/iris.csv&#39; 5.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris_r.csv&quot;) # Rename columns df &lt;- df %&gt;% rename(sepal_length = Sepal.Length, sepal_width = Sepal.Width, petal_length = Petal.Length, petal_width = Petal.Width, species = Species) # Save the standardized dataset write_csv(df, &quot;data/iris.csv&quot;) print(&quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot;) [1] &quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot; "],["how-to-load-a-dataset-in-python-and-r.html", "6 How to load a dataset in Python and R? 6.1 Explanation 6.2 Python Code 6.3 R Code", " 6 How to load a dataset in Python and R? 6.1 Explanation Loading a dataset is one of the first steps in any data analysis project. In this case, we’ll load the Iris dataset, a popular dataset for beginner data science projects, in both Python and R. The dataset has been saved as iris.csv in your data folder. We will use pandas in Python and readr in R to load the dataset into a dataframe. In Python we will use the pandas library, which is a powerful tool for data manipulation and analysis. The read_csv() function in pandas will allow us to read the iris.csv file into a dataframe. In R we will use the readr package, which provides modern and faster alternatives to base R functions. The read_csv() function in readr is similar to pandas in Python and offers a streamlined approach for loading CSV files. 6.2 Python Code import pandas as pd # Load the dataset iris = pd.read_csv(&#39;data/iris.csv&#39;) # Show the first few rows print(iris.head()) sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 6.3 R Code # Load necessary library library(readr) # Load dataset from CSV file df &lt;- read_csv(&quot;data/iris.csv&quot;) # Display the first few rows head(df) # A tibble: 6 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa "],["how-to-explore-a-dataset-in-python-and-r.html", "7 How to Explore a Dataset in Python and R? 7.1 Explanation 7.2 Python Code 7.3 R Code", " 7 How to Explore a Dataset in Python and R? 7.1 Explanation After loading a dataset, it is important to explore its structure, summary statistics, and key properties before performing any analysis. This helps in understanding the data and identifying potential issues such as missing values or outliers. In this section, we will: View the structure of the dataset. Get summary statistics. Check for missing values. 7.2 Python Code import pandas as pd # Load the dataset iris = pd.read_csv(&#39;data/iris.csv&#39;) # Display basic information about the dataset print(&quot;Dataset Information:&quot;) print(iris.info()) # Show summary statistics print(&quot;\\nSummary Statistics:&quot;) print(iris.describe()) # Check for missing values print(&quot;\\nMissing Values:&quot;) print(iris.isnull().sum()) Dataset Information: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 sepal_length 150 non-null float64 1 sepal_width 150 non-null float64 2 petal_length 150 non-null float64 3 petal_width 150 non-null float64 4 target 150 non-null int64 5 species 150 non-null object dtypes: float64(4), int64(1), object(1) memory usage: 7.2+ KB None Summary Statistics: sepal_length sepal_width petal_length petal_width target count 150.000000 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 1.000000 std 0.828066 0.435866 1.765298 0.762238 0.819232 min 4.300000 2.000000 1.000000 0.100000 0.000000 25% 5.100000 2.800000 1.600000 0.300000 0.000000 50% 5.800000 3.000000 4.350000 1.300000 1.000000 75% 6.400000 3.300000 5.100000 1.800000 2.000000 max 7.900000 4.400000 6.900000 2.500000 2.000000 Missing Values: sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 species 0 dtype: int64 7.3 R Code # Load necessary library library(readr) library(dplyr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Display the structure of the dataset cat(&quot;Dataset Structure:\\n&quot;) Dataset Structure: str(df) spc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ sepal_length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ petal_length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ species : chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... - attr(*, &quot;spec&quot;)= .. cols( .. sepal_length = col_double(), .. sepal_width = col_double(), .. petal_length = col_double(), .. petal_width = col_double(), .. species = col_character() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Show summary statistics cat(&quot;\\nSummary Statistics:\\n&quot;) Summary Statistics: summary(df) sepal_length sepal_width petal_length petal_width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 species Length:150 Class :character Mode :character # Check for missing values cat(&quot;\\nMissing Values:\\n&quot;) Missing Values: colSums(is.na(df)) sepal_length sepal_width petal_length petal_width species 0 0 0 0 0 "],["how-to-handle-missing-data-in-python-and-r.html", "8 How to Handle Missing Data in Python and R? 8.1 Explanation 8.2 Python Code 8.3 R Code", " 8 How to Handle Missing Data in Python and R? 8.1 Explanation Handling missing data is one of the most important steps in data cleaning. In this section, we’ll explore how to handle missing values in a dataset. There are several strategies for handling missing data, such as: Removing missing values Imputing missing values (filling them with a specific value or a calculated statistic) In this guide, we will focus on removing missing values, though you can also explore imputation depending on your data and goals. 8.2 Python Code In Python, we can use pandas to detect and handle missing data. The isna() and dropna() functions are commonly used for this task. import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) df # Check for missing values print(df.isna().sum()) # Remove rows with missing data df_cleaned = df.dropna() # Check the cleaned data print(df_cleaned.head()) sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 species 0 dtype: int64 sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 8.3 R Code In R, we can use the is.na() function to detect missing values, and the na.omit() function to remove them. library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Check for missing values missing_values &lt;- colSums(is.na(df)) print(missing_values) sepal_length sepal_width petal_length petal_width species 0 0 0 0 0 # Remove rows with missing data df_cleaned &lt;- na.omit(df) # Check the cleaned data head(df_cleaned) # A tibble: 6 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa Handling missing data properly ensures that your analysis is accurate and that missing values do not introduce bias into your model or analysis. "],["how-to-filter-data-in-python-and-r.html", "9 How to Filter Data in Python and R? 9.1 Explanation 9.2 Python Code 9.3 R Code", " 9 How to Filter Data in Python and R? 9.1 Explanation Filtering data is a common task in data science. It allows you to select specific rows based on conditions or criteria, helping you focus on the subset of data that is most relevant to your analysis. In this section, we will learn how to filter data using conditions in both Python and R. We’ll demonstrate filtering rows based on specific column values. 9.2 Python Code In Python, we use pandas to filter data. The loc[] method is useful for selecting rows based on specific conditions. import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Filter rows where species is &#39;setosa&#39; setosa_df = df[df[&#39;species&#39;] == &#39;setosa&#39;] # Show the first few rows of the filtered data print(setosa_df.head()) sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 9.3 R Code In R, we can use the dplyr package to filter rows based on specific conditions. The filter() function is used for this purpose. library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Filter rows where species is &#39;setosa&#39; setosa_df &lt;- df %&gt;% filter(species == &#39;setosa&#39;) # Show the first few rows of the filtered data head(setosa_df) # A tibble: 6 × 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa Filtering data allows you to narrow down your dataset and focus on specific insights or subsets of interest. In real-world projects, filtering is often one of the first steps in analyzing a dataset. "],["how-to-group-data-in-python-and-r.html", "10 How to Group Data in Python and R? 10.1 Explanation 10.2 Python Code 10.3 R Code", " 10 How to Group Data in Python and R? 10.1 Explanation Grouping data is a common task when summarizing and analyzing datasets. In this section, we’ll group the Iris dataset by the Species column and calculate summary statistics (e.g., mean) for each group. In Python, we use pandas for this task, while in R we use dplyr. Let’s explore how to group the data by species and calculate the mean of each numeric column. 10.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Group by &#39;Species&#39; and calculate the mean for each group grouped_df = df.groupby(&#39;species&#39;).mean() # Display the grouped data print(grouped_df) sepal_length sepal_width petal_length petal_width target species setosa 5.006 3.428 1.462 0.246 0.0 versicolor 5.936 2.770 4.260 1.326 1.0 virginica 6.588 2.974 5.552 2.026 2.0 10.3 R Code library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Group by &#39;species&#39; and calculate the mean for each group grouped_df &lt;- df %&gt;% group_by(species) %&gt;% summarise(across(where(is.numeric), mean)) # Display the grouped data print(grouped_df) # A tibble: 3 × 5 species sepal_length sepal_width petal_length petal_width &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 setosa 5.01 3.43 1.46 0.246 2 versicolor 5.94 2.77 4.26 1.33 3 virginica 6.59 2.97 5.55 2.03 "],["how-to-aggregate-data-in-python-and-r.html", "11 How to Aggregate Data in Python and R? 11.1 Explanation 11.2 Python Code 11.3 R Code", " 11 How to Aggregate Data in Python and R? 11.1 Explanation Aggregation helps summarize datasets by computing statistics such as mean, median, count, or sum for different groups. This is useful when analyzing patterns within the dataset. For example, in the iris dataset, we can find the average sepal width per species to compare flower characteristics. 11.2 Python Code import pandas as pd # Load the dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Aggregate: Calculate mean sepal width per species agg_df = df.groupby(&quot;species&quot;)[&quot;sepal_width&quot;].mean().reset_index() # Save the aggregated data agg_df.to_csv(&quot;data/iris_aggregated_py.csv&quot;, index=False) # Display the result print(agg_df) species sepal_width 0 setosa 3.428 1 versicolor 2.770 2 virginica 2.974 11.3 R Code # Load necessary libraries library(readr) library(dplyr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Aggregate: Calculate mean sepal width per species agg_df &lt;- df %&gt;% group_by(species) %&gt;% summarise(mean_sepal_width = mean(sepal_width, na.rm = TRUE)) # Save the aggregated data write_csv(agg_df, &quot;data/iris_aggregated_r.csv&quot;) # Display the result print(agg_df) # A tibble: 3 × 2 species mean_sepal_width &lt;chr&gt; &lt;dbl&gt; 1 setosa 3.43 2 versicolor 2.77 3 virginica 2.97 "],["how-to-split-a-dataset-in-python-and-r.html", "12 How to Split a Dataset in Python and R? 12.1 Explanation 12.2 Python Code 12.3 R Code", " 12 How to Split a Dataset in Python and R? 12.1 Explanation Splitting a dataset into multiple parts is useful when you want to work with subsets of data. In this case, we will split the iris dataset into two parts: x - iris_part1.csv: Contains columns sepal_length, sepal_width, and species. - iris_part2.csv: Contains columns petal_length, petal_width, and species. These parts will later be merged based on the species column. 12.2 Python Code import pandas as pd # Load the iris dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Split into two parts part1 = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;species&#39;]] part2 = df[[&#39;petal_length&#39;, &#39;petal_width&#39;, &#39;species&#39;]] # Save the parts as separate CSV files part1.to_csv(&quot;data/iris_part1.csv&quot;, index=False) part2.to_csv(&quot;data/iris_part2.csv&quot;, index=False) # Display a message to confirm print(&quot;Parts saved as iris_part1.csv and iris_part2.csv&quot;) Parts saved as iris_part1.csv and iris_part2.csv 12.3 R Code # Load necessary libraries library(readr) # Load the iris dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Split into two parts part1 &lt;- df[, c(&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;species&quot;)] part2 &lt;- df[, c(&quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;)] # Save the parts as separate CSV files write_csv(part1, &quot;data/iris_part1.csv&quot;) write_csv(part2, &quot;data/iris_part2.csv&quot;) # Display a message to confirm cat(&quot;Parts saved as iris_part1.csv and iris_part2.csv\\n&quot;) Parts saved as iris_part1.csv and iris_part2.csv Now that we have created the two parts, we can proceed to merge them using the steps outlined previously. "],["how-to-merge-datasets-in-python-and-r.html", "13 How to Merge Datasets in Python and R? 13.1 Explanation 13.2 Python Code 13.3 R Code", " 13 How to Merge Datasets in Python and R? 13.1 Explanation Merging datasets is a common task when working with multiple data sources. In the iris dataset, we may want to combine different subsets of data based on a common column, such as the species. In this example, we assume there are two datasets: iris_part1.csv (contains sepal_length, sepal_width, and species) iris_part2.csv (contains petal_length, petal_width, and species) We will merge them on the species column. 13.2 Python Code import pandas as pd # Load the two parts of the iris dataset (with renamed columns) part1 = pd.read_csv(&quot;data/iris_part1.csv&quot;) part2 = pd.read_csv(&quot;data/iris_part2.csv&quot;) # Merge the datasets based on the &#39;species&#39; column merged_df = pd.merge(part1, part2, on=&#39;species&#39;) # Save the merged dataset as a new CSV file merged_df.to_csv(&quot;data/iris_merged.csv&quot;, index=False) # Display the first few rows of the merged dataset print(merged_df.head()) sepal_length sepal_width species petal_length petal_width 0 5.1 3.5 setosa 1.4 0.2 1 5.1 3.5 setosa 1.4 0.2 2 5.1 3.5 setosa 1.3 0.2 3 5.1 3.5 setosa 1.5 0.2 4 5.1 3.5 setosa 1.4 0.2 13.3 R Code # Load necessary library library(readr) # Load the two parts of the iris dataset (with renamed columns) part1 &lt;- read_csv(&quot;data/iris_part1.csv&quot;) part2 &lt;- read_csv(&quot;data/iris_part2.csv&quot;) # Merge the datasets based on the &#39;species&#39; column merged_df &lt;- merge(part1, part2, by = &quot;species&quot;) # Save the merged dataset as a new CSV file write_csv(merged_df, &quot;data/iris_merged.csv&quot;) # Display the first few rows of the merged dataset head(merged_df) species sepal_length sepal_width petal_length petal_width 1 setosa 5.1 3.5 1.4 0.2 2 setosa 5.1 3.5 1.4 0.2 3 setosa 5.1 3.5 1.3 0.2 4 setosa 5.1 3.5 1.5 0.2 5 setosa 5.1 3.5 1.4 0.2 6 setosa 5.1 3.5 1.7 0.4 "],["how-to-create-a-bar-plot-in-python-and-r.html", "14 How to Create a Bar Plot in Python and R? 14.1 Explanation 14.2 Python Code 14.3 R Code", " 14 How to Create a Bar Plot in Python and R? 14.1 Explanation A bar plot is used to visualize categorical data with rectangular bars representing the frequency or value of categories. 14.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a bar plot sns.countplot(x=&quot;species&quot;, data=df) plt.title(&quot;Bar Plot of Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Count&quot;) plt.show() 14.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a bar plot with fill color based on species ggplot(df, aes(x=species, fill=species)) + geom_bar() + theme_minimal() + ggtitle(&quot;Bar Plot of Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Count&quot;) "],["how-to-create-a-histogram-in-python-and-r.html", "15 How to Create a Histogram in Python and R? 15.1 Explanation 15.2 Python Code 15.3 R Code", " 15 How to Create a Histogram in Python and R? 15.1 Explanation A histogram displays the distribution of a continuous variable by dividing the data into bins or intervals. The height of each bar represents the frequency of values within that bin. 15.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a histogram sns.histplot(df[&#39;sepal_length&#39;], kde=True, bins=10) plt.title(&quot;Histogram of Sepal Length&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() 15.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a histogram ggplot(df, aes(x=sepal_length)) + geom_histogram(bins=10, fill=&quot;skyblue&quot;, color=&quot;black&quot;) + theme_minimal() + ggtitle(&quot;Histogram of Sepal Length&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Frequency&quot;) "],["how-to-create-a-scatter-plot-in-python-and-r.html", "16 How to Create a Scatter Plot in Python and R? 16.1 Explanation 16.2 Python Code 16.3 R Code", " 16 How to Create a Scatter Plot in Python and R? 16.1 Explanation A scatter plot is used to show the relationship between two continuous variables. Each point represents an observation in the dataset. 16.2 Python Code import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) df[&#39;species&#39;] = df[&#39;species&#39;].astype(&#39;category&#39;) # Ensure categorical variable # Set a better background sns.set_style(&quot;whitegrid&quot;) # Create a scatter plot with improved coloring ax = sns.scatterplot(x=&#39;sepal_length&#39;, y=&#39;sepal_width&#39;, hue=&#39;species&#39;, data=df, palette=&#39;Set1&#39;) # Ensure legend is displayed plt.legend(title=&quot;Species&quot;) # Add labels and title plt.title(&quot;Scatter Plot of Sepal Length vs Sepal Width by Species&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Sepal Width&quot;) # Show the plot plt.show() 16.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a scatter plot colored by species ggplot(df, aes(x=sepal_length, y=sepal_width, color=species)) + geom_point() + theme_minimal() + ggtitle(&quot;Scatter Plot of Sepal Length vs Sepal Width by Species&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Sepal Width&quot;) "],["how-to-create-a-line-plot-in-python-and-r.html", "17 How to Create a Line Plot in Python and R? 17.1 Explanation 17.2 Python Code 17.3 R Code", " 17 How to Create a Line Plot in Python and R? 17.1 Explanation A line plot is used to display the relationship between two continuous variables, where data points are connected by lines to show trends over time or other continuous variables. 17.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a line plot sns.lineplot(x=&#39;sepal_length&#39;, y=&#39;sepal_width&#39;, data=df) plt.title(&quot;Line Plot of Sepal Length vs Sepal Width&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Sepal Width&quot;) plt.show() 17.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a line plot with a specific color ggplot(df, aes(x=sepal_length, y=sepal_width)) + geom_line(color=&quot;steelblue&quot;) + # Set the line color to steelblue theme_minimal() + ggtitle(&quot;Line Plot of Sepal Length vs Sepal Width&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Sepal Width&quot;) "],["how-to-create-a-box-plot-in-python-and-r.html", "18 How to Create a Box Plot in Python and R? 18.1 Explanation 18.2 Python Code 18.3 R Code", " 18 How to Create a Box Plot in Python and R? 18.1 Explanation A box plot (or box-and-whisker plot) is used to display the distribution of a continuous variable, highlighting the median, quartiles, and potential outliers. 18.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a box plot sns.boxplot(x=&#39;species&#39;, y=&#39;sepal_length&#39;, data=df) plt.title(&quot;Box Plot of Sepal Length by Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Sepal Length&quot;) plt.show() 18.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a box plot ggplot(df, aes(x=species, y=sepal_length, fill=species)) + geom_boxplot() + theme_minimal() + ggtitle(&quot;Box Plot of Sepal Length by Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Sepal Length&quot;) "],["how-to-create-a-heatmap-in-python-and-r.html", "19 How to Create a Heatmap in Python and R? 19.1 Explanation 19.2 Python Code 19.3 R Code", " 19 How to Create a Heatmap in Python and R? 19.1 Explanation A heatmap is used to display data in matrix form, where individual values are represented by colors. It’s often used to visualize correlation matrices or other tabular data. 19.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Select only numeric columns for correlation numeric_df = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]) # Create a correlation matrix corr = numeric_df.corr() # Create a heatmap with a specified range for color scale sns.heatmap(corr, annot=True, cmap=&quot;coolwarm&quot;, vmin=-1, vmax=1) plt.title(&quot;Heatmap of Iris Dataset Correlations&quot;) plt.show() 19.3 R Code # Import libraries library(readr) # For reading CSV library(corrplot) # For correlation matrix visualization library(dplyr) # For data manipulation # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Select only numeric columns for correlation numeric_df &lt;- df %&gt;% select(where(is.numeric)) # Create a correlation matrix corr &lt;- cor(numeric_df) # Create a heatmap using corrplot with adjusted x-axis label position corrplot(corr, method = &quot;color&quot;, col = colorRampPalette(c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;))(200), addCoef.col = &quot;black&quot;, number.cex = 0.7, title = &quot;Heatmap of Iris Dataset Correlations&quot;, mar = c(0, 0, 2, 0), # Adjust margin size to create space for the title las = 1) # Rotate x-axis labels for better spacing "],["how-to-create-a-pair-plot-in-python-and-r.html", "20 How to Create a Pair Plot in Python and R? 20.1 Explanation 20.2 Python Code 20.3 R Code", " 20 How to Create a Pair Plot in Python and R? 20.1 Explanation A pair plot is used to visualize pairwise relationships between multiple variables. It’s particularly useful for understanding the correlations between multiple continuous variables. 20.2 Python Code import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import warnings # Suppress warnings # Hide warnings warnings.filterwarnings(&quot;ignore&quot;) # Load dataset from CSV df = pd.read_csv(&quot;data/iris.csv&quot;) # Exclude the target column and plot pairplot sns.pairplot(df, hue=&quot;species&quot;) # Adjust layout to prevent overlapping plt.tight_layout() # Show the plot plt.show() 20.3 R Code # Ensure required packages are installed and loaded if (!require(GGally)) install.packages(&quot;GGally&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) if (!require(ggplot2)) install.packages(&quot;ggplot2&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) if (!require(readr)) install.packages(&quot;readr&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) library(GGally) library(ggplot2) library(readr) # Load dataset from CSV df &lt;- read_csv(&quot;data/iris.csv&quot;) # Plot pairplot excluding the target column but using it as hue ggpairs(df, aes(color = species)) + theme_minimal() "],["how-to-create-a-violin-plot-in-python-and-r.html", "21 How to Create a Violin Plot in Python and R? 21.1 Explanation 21.2 Python Code 21.3 R Code", " 21 How to Create a Violin Plot in Python and R? 21.1 Explanation A violin plot combines aspects of a box plot and a kernel density plot. It provides a detailed view of the distribution of a continuous variable for different categories. 21.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a violin plot sns.violinplot(x=&#39;species&#39;, y=&#39;sepal_length&#39;, data=df) plt.title(&quot;Violin Plot of Sepal Length by Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Sepal Length&quot;) plt.show() 21.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a violin plot with grouping by color ggplot(df, aes(x=species, y=sepal_length, fill=species)) + geom_violin() + theme_minimal() + ggtitle(&quot;Violin Plot of Sepal Length by Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Sepal Length&quot;) "],["how-to-perform-statistical-analysis-in-python-and-r.html", "22 How to Perform Statistical Analysis in Python and R? 22.1 Explanation 22.2 Python Code 22.3 R Code", " 22 How to Perform Statistical Analysis in Python and R? 22.1 Explanation Statistical analysis helps us understand the characteristics of our dataset, identify patterns, and make data-driven decisions. In this section, we will cover basic statistical measures such as mean, median, variance, and correlation. 22.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Summary statistics summary_stats = df.describe() # Calculate variance for numerical columns variance = df.var(numeric_only=True) # Calculate correlation between numerical variables correlation = df.corr(numeric_only=True) # Display results print(&quot;Summary Statistics:\\n&quot;, summary_stats) print(&quot;\\nVariance:\\n&quot;, variance) print(&quot;\\nCorrelation:\\n&quot;, correlation) Summary Statistics: sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 Variance: sepal_length 0.685694 sepal_width 0.189979 petal_length 3.116278 petal_width 0.581006 dtype: float64 Correlation: sepal_length sepal_width petal_length petal_width sepal_length 1.000000 -0.117570 0.871754 0.817941 sepal_width -0.117570 1.000000 -0.428440 -0.366126 petal_length 0.871754 -0.428440 1.000000 0.962865 petal_width 0.817941 -0.366126 0.962865 1.000000 22.3 R Code # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Summary statistics summary_stats &lt;- summary(df) # Calculate variance for numerical columns variance &lt;- apply(df[, 1:4], 2, var) # Calculate correlation between numerical variables correlation &lt;- cor(df[, 1:4]) # Display results print(&quot;Summary Statistics:&quot;) [1] &quot;Summary Statistics:&quot; print(summary_stats) sepal_length sepal_width petal_length petal_width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 species Length:150 Class :character Mode :character print(&quot;\\nVariance:&quot;) [1] &quot;\\nVariance:&quot; print(variance) sepal_length sepal_width petal_length petal_width 0.6856935 0.1899794 3.1162779 0.5810063 print(&quot;\\nCorrelation:&quot;) [1] &quot;\\nCorrelation:&quot; print(correlation) sepal_length sepal_width petal_length petal_width sepal_length 1.0000000 -0.1175698 0.8717538 0.8179411 sepal_width -0.1175698 1.0000000 -0.4284401 -0.3661259 petal_length 0.8717538 -0.4284401 1.0000000 0.9628654 petal_width 0.8179411 -0.3661259 0.9628654 1.0000000 "],["how-to-calculate-skewness-and-kurtosis-in-python-and-r.html", "23 How to Calculate Skewness and Kurtosis in Python and R? 23.1 Explanation 23.2 Python Code 23.3 R Code", " 23 How to Calculate Skewness and Kurtosis in Python and R? 23.1 Explanation Skewness and kurtosis help us understand the distribution of data. - Skewness measures the asymmetry of the data distribution. A skewness of 0 indicates a perfectly symmetric distribution. - Kurtosis measures the “tailedness” of the distribution. A normal distribution has a kurtosis of 3. Values greater than 3 indicate heavy tails, while values less than 3 indicate light tails. 23.2 Python Code import pandas as pd from scipy.stats import skew, kurtosis # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Compute skewness skewness = df.iloc[:, :-1].apply(skew) # Compute kurtosis kurt = df.iloc[:, :-1].apply(kurtosis) # Display results print(&quot;Skewness:\\n&quot;, skewness) print(&quot;\\nKurtosis:\\n&quot;, kurt) Skewness: sepal_length 0.311753 sepal_width 0.315767 petal_length -0.272128 petal_width -0.101934 dtype: float64 Kurtosis: sepal_length -0.573568 sepal_width 0.180976 petal_length -1.395536 petal_width -1.336067 dtype: float64 23.3 R Code # Check and load necessary libraries from CRAN mirror if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;, dependencies = TRUE, repos = &quot;https://cloud.r-project.org/&quot;) if(!require(e1071)) install.packages(&quot;e1071&quot;, dependencies = TRUE, repos = &quot;https://cloud.r-project.org/&quot;) library(tidyverse) library(e1071) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Compute skewness and kurtosis skewness_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), skewness)) kurtosis_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), kurtosis)) # Display results print(&quot;Skewness:&quot;) [1] &quot;Skewness:&quot; print(skewness_values) # A tibble: 1 × 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.309 0.313 -0.269 -0.101 print(&quot;Kurtosis:&quot;) [1] &quot;Kurtosis:&quot; print(kurtosis_values) # A tibble: 1 × 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -0.606 0.139 -1.42 -1.36 "],["how-to-perform-a-t-test-in-python-and-r.html", "24 How to Perform a t-test in Python and R? 24.1 Explanation 24.2 Python Code", " 24 How to Perform a t-test in Python and R? 24.1 Explanation t-tests are used to compare the means of two groups and determine whether they are significantly different from each other. In the iris dataset, we can compare the sepal length of two species to see if their means differ significantly. There are different types of t-tests: Independent t-test: Compares means between two independent groups. Paired t-test: Compares means from the same group at different time points. 24.2 Python Code In Python, we use scipy.stats.ttest_ind() for an independent t-test. import pandas as pd from scipy import stats # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Filter two species for comparison setosa = df[df[&#39;species&#39;] == &#39;setosa&#39;][&#39;sepal_length&#39;] versicolor = df[df[&#39;species&#39;] == &#39;versicolor&#39;][&#39;sepal_length&#39;] # Perform independent t-test t_stat, p_value = stats.ttest_ind(setosa, versicolor) print(f&quot;t-statistic: {t_stat}, p-value: {p_value}&quot;) t-statistic: -10.52098626754911, p-value: 8.985235037487079e-18 "],["how-to-compute-the-mean-median-and-mode-of-a-dataset.html", "25 How to compute the mean, median, and mode of a dataset? 25.1 Explanation 25.2 Python Code 25.3 R Code", " 25 How to compute the mean, median, and mode of a dataset? 25.1 Explanation Mean: The average of all values in the dataset. Median: The middle value when the data is sorted. Mode: The value that appears most frequently in the dataset. 25.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Compute mean, median, and mode mean_values = df.drop(columns=[&quot;species&quot;]).mean() median_values = df.drop(columns=[&quot;species&quot;]).median() mode_values = df.drop(columns=[&quot;species&quot;]).mode().iloc[0] # Display results print(&quot;Mean:\\n&quot;) print(mean_values) print(&quot;\\nMedian:\\n&quot;) print(median_values) print(&quot;\\nMode:\\n&quot;) print(mode_values) Mean: sepal_length 5.843333 sepal_width 3.057333 petal_length 3.758000 petal_width 1.199333 dtype: float64 Median: sepal_length 5.80 sepal_width 3.00 petal_length 4.35 petal_width 1.30 dtype: float64 Mode: sepal_length 5.0 sepal_width 3.0 petal_length 1.4 petal_width 0.2 Name: 0, dtype: float64 25.3 R Code # Load necessary libraries library(tidyverse) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Compute mean, median, and mode mean_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), mean)) median_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), median)) mode_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), ~ names(sort(table(.), decreasing = TRUE))[1])) # Display results print(&quot;Mean:&quot;) [1] &quot;Mean:&quot; print(mean_values) # A tibble: 1 × 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5.84 3.06 3.76 1.20 print(&quot;Median:&quot;) [1] &quot;Median:&quot; print(median_values) # A tibble: 1 × 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5.8 3 4.35 1.3 print(&quot;Mode:&quot;) [1] &quot;Mode:&quot; print(mode_values) # A tibble: 1 × 4 sepal_length sepal_width petal_length petal_width &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 5 3 1.4 0.2 "],["what-is-the-difference-between-an-f-test-and-an-anova-test.html", "26 What is the Difference Between an F-test and an ANOVA Test? 26.1 Overview 26.2 Key Aspects 26.3 Key Differences 26.4 What If Variances Are Not Equal?", " 26 What is the Difference Between an F-test and an ANOVA Test? 26.1 Overview Both the F-test and ANOVA use the F-statistic, but they serve different purposes in statistical analysis. 26.2 Key Aspects Aspect F-test (Variance Comparison) ANOVA (Mean Comparison) Purpose Compares the variances of two groups. Compares the means of three or more groups. Hypotheses - \\(H_0\\): Variances are equal. - \\(H_a\\): Variances are different. - \\(H_0\\): All group means are equal. - \\(H_a\\): At least one mean is different. When to Use? Before a t-test, to check variance equality. When analyzing differences among multiple groups. Test Statistic \\(F = \\frac{\\sigma_1^2}{\\sigma_2^2}\\) (Ratio of variances) \\(F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}\\) Python Function levene() or bartlett() from scipy.stats. f_oneway() from scipy.stats (for means). R Function var.test(group1, group2). aov(response ~ group, data = df). 26.3 Key Differences The F-test is used to compare variances between two groups. ANOVA is used to compare means among three or more groups. The F-test is often used before ANOVA to check if the assumption of equal variances holds. 26.4 What If Variances Are Not Equal? If the assumption of equal variances is violated, consider using: - Welch’s ANOVA, which does not assume equal variances. - Non-parametric tests, such as the Kruskal-Wallis test (for comparing medians). "],["how-to-perform-an-f-test-in-python-and-r.html", "27 How to Perform an F-test in Python and R? 27.1 Explanation 27.2 Python Code 27.3 R Code", " 27 How to Perform an F-test in Python and R? 27.1 Explanation An F-test is used to compare the variances of two independent groups. It helps determine if the groups have equal variances, which is important in statistical tests like t-tests and ANOVA. The null hypothesis (\\(H_0\\)) assumes that the variances of the two groups are equal, while the alternative hypothesis (\\(H_a\\)) states that they are different. The F-statistic for the F-test is calculated as: \\[ F = \\frac{\\text{variance of group 1}}{\\text{variance of group 2}} \\] If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that the variances are significantly different. 27.2 Python Code import pandas as pd from scipy.stats import levene # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select two species for comparison group1 = df[df[&quot;species&quot;] == &quot;setosa&quot;][&quot;sepal_length&quot;] group2 = df[df[&quot;species&quot;] == &quot;versicolor&quot;][&quot;sepal_length&quot;] # Perform F-test for variances (Levene&#39;s test) f_stat, p_value = levene(group1, group2) # Display results print(f&quot;F-statistic: {f_stat:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject H0: The variances of Setosa and Versicolor are significantly different.&quot;) else: print(&quot;Fail to reject H0: No significant difference in variances.&quot;) F-statistic: 8.1727 P-value: 0.0052 Reject H0: The variances of Setosa and Versicolor are significantly different. 27.3 R Code # Load necessary package if (!require(car)) install.packages(&quot;car&quot;, repos = &quot;https://cloud.r-project.org&quot;) library(car) # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Subset data for two species group1 &lt;- df[df$species == &quot;setosa&quot;, &quot;sepal_length&quot;] group2 &lt;- df[df$species == &quot;versicolor&quot;, &quot;sepal_length&quot;] # Perform F-test for variances (Levene&#39;s test) var_test_result &lt;- var.test(group1, group2) print(var_test_result) F test to compare two variances data: group1 and group2 F = 0.46634, num df = 49, denom df = 49, p-value = 0.008657 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.2646385 0.8217841 sample estimates: ratio of variances 0.4663429 "],["how-to-perform-an-anova-test-in-python-and-r.html", "28 How to Perform an ANOVA Test in Python and R? 28.1 Explanation 28.2 Python Code 28.3 R Code", " 28 How to Perform an ANOVA Test in Python and R? 28.1 Explanation An ANOVA (Analysis of Variance) test is used to compare the means of three or more groups. It helps to determine whether there are any statistically significant differences between the means of the groups. The null hypothesis ((H_0)) assumes that all group means are equal, while the alternative hypothesis ((H_a)) states that at least one mean is different. The F-statistic in ANOVA is calculated as: \\[ F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}} \\] If the p-value is small (typically ( p &lt; 0.05 )), we reject the null hypothesis and conclude that at least one group mean is significantly different. 28.2 Python Code import pandas as pd from scipy.stats import f_oneway # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select groups for comparison group1 = df[df[&quot;species&quot;] == &quot;setosa&quot;][&quot;sepal_length&quot;] group2 = df[df[&quot;species&quot;] == &quot;versicolor&quot;][&quot;sepal_length&quot;] group3 = df[df[&quot;species&quot;] == &quot;virginica&quot;][&quot;sepal_length&quot;] # Perform ANOVA f_stat, p_value = f_oneway(group1, group2, group3) # Display results print(f&quot;F-statistic: {f_stat:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject H0: At least one group mean is significantly different.&quot;) else: print(&quot;Fail to reject H0: No significant difference in group means.&quot;) F-statistic: 119.2645 P-value: 0.0000 Reject H0: At least one group mean is significantly different. 28.3 R Code # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Perform ANOVA (sepal length by species) anova_result &lt;- aov(sepal_length ~ species, data = df) summary(anova_result) Df Sum Sq Mean Sq F value Pr(&gt;F) species 2 63.21 31.606 119.3 &lt;2e-16 *** Residuals 147 38.96 0.265 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["how-to-perform-a-chi-square-test-in-python-and-r.html", "29 How to Perform a Chi-Square Test in Python and R? 29.1 Explanation 29.2 Python Code 29.3 R Code", " 29 How to Perform a Chi-Square Test in Python and R? 29.1 Explanation A Chi-Square Test is used to determine whether there is a significant association between two categorical variables. It compares the observed frequencies in each category to the frequencies we would expect if the variables were independent. The Chi-Square Statistic is calculated as: \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} \\] Where: - \\(O_i\\) is the observed frequency in category \\(i\\), - \\(E_i\\) is the expected frequency in category \\(i\\). The null hypothesis (\\(H_0\\)) assumes that there is no association between the variables (i.e., the variables are independent), while the alternative hypothesis (\\(H_a\\)) states that there is an association between them. If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that there is a significant association between the variables. 29.2 Python Code Chi-Square Test - Association Between Categorical Variables import pandas as pd from scipy.stats import chi2_contingency # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Create a contingency table for &#39;species&#39; and &#39;sepal_width&#39; (categorical grouping) contingency_table = pd.crosstab(df[&#39;species&#39;], pd.cut(df[&#39;sepal_width&#39;], bins=3)) # Perform Chi-Square Test chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table) # Display results print(f&quot;Chi-Square Statistic: {chi2_stat:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) print(f&quot;Degrees of Freedom: {dof}&quot;) print(f&quot;Expected Frequencies: \\n{expected}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject H0: There is a significant association between species and sepal width.&quot;) else: print(&quot;Fail to reject H0: No significant association between species and sepal width.&quot;) Chi-Square Statistic: 45.1247 P-value: 0.0000 Degrees of Freedom: 4 Expected Frequencies: [[15.66666667 29.33333333 5. ] [15.66666667 29.33333333 5. ] [15.66666667 29.33333333 5. ]] Reject H0: There is a significant association between species and sepal width. 29.3 R Code Chi-Square Test - Association Between Categorical Variables # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create a contingency table for &#39;species&#39; and &#39;sepal_width&#39; (categorical grouping) df$sepal_width_cat &lt;- cut(df$sepal_width, breaks = 3) contingency_table &lt;- table(df$species, df$sepal_width_cat) # Perform Chi-Square Test chi2_result &lt;- chisq.test(contingency_table) # Display results print(chi2_result) Pearson&#39;s Chi-squared test data: contingency_table X-squared = 45.125, df = 4, p-value = 3.746e-09 # Interpretation if (chi2_result$p.value &lt; 0.05) { print(&quot;Reject H0: There is a significant association between species and sepal width.&quot;) } else { print(&quot;Fail to reject H0: No significant association between species and sepal width.&quot;) } [1] &quot;Reject H0: There is a significant association between species and sepal width.&quot; "],["how-to-perform-a-pearson-correlation-test-in-python-and-r.html", "30 How to Perform a Pearson Correlation Test in Python and R? 30.1 Explanation 30.2 Python Code 30.3 R Code", " 30 How to Perform a Pearson Correlation Test in Python and R? 30.1 Explanation The Pearson Correlation Test is used to determine the linear relationship between two continuous variables. It measures the strength and direction of the relationship, with a correlation coefficient (\\(r\\)) ranging from -1 to 1: - \\(r = 1\\) indicates a perfect positive linear relationship. - \\(r = -1\\) indicates a perfect negative linear relationship. - \\(r = 0\\) indicates no linear relationship. The null hypothesis (\\(H_0\\)) assumes that there is no linear correlation between the two variables, while the alternative hypothesis (\\(H_a\\)) states that there is a linear correlation. If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that there is a significant linear relationship between the two variables. 30.2 Python Code import pandas as pd from scipy.stats import pearsonr # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select two variables for correlation test x = df[&#39;sepal_length&#39;] y = df[&#39;sepal_width&#39;] # Perform Pearson Correlation Test corr_coefficient, p_value = pearsonr(x, y) # Display results print(f&quot;Pearson Correlation Coefficient: {corr_coefficient:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject H0: There is a significant linear correlation between sepal length and sepal width.&quot;) else: print(&quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot;) Pearson Correlation Coefficient: -0.1176 P-value: 0.1519 Fail to reject H0: No significant linear correlation between sepal length and sepal width. 30.3 R Code # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Select two variables for correlation test x &lt;- df$sepal_length y &lt;- df$sepal_width # Perform Pearson Correlation Test cor_result &lt;- cor.test(x, y) # Display results print(cor_result) Pearson&#39;s product-moment correlation data: x and y t = -1.4403, df = 148, p-value = 0.1519 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.27269325 0.04351158 sample estimates: cor -0.1175698 # Interpretation if (cor_result$p.value &lt; 0.05) { print(&quot;Reject H0: There is a significant linear correlation between sepal length and sepal width.&quot;) } else { print(&quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot;) } [1] &quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot; "],["how-to-perform-a-simple-linear-regression-in-python-and-r.html", "31 How to Perform a Simple Linear Regression in Python and R? 31.1 Explanation 31.2 Python Code 31.3 R Code", " 31 How to Perform a Simple Linear Regression in Python and R? 31.1 Explanation Simple Linear Regression is used to model the relationship between a dependent variable (\\(Y\\)) and an independent variable (\\(X\\)) by fitting a linear equation to observed data. The goal is to find the best-fitting line, represented by the equation: \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\] Where: - \\(Y\\) is the dependent variable, - \\(X\\) is the independent variable, - \\(\\beta_0\\) is the intercept, - \\(\\beta_1\\) is the slope, - \\(\\epsilon\\) is the error term. The null hypothesis (\\(H_0\\)) assumes that the slope of the regression line is zero, meaning there is no linear relationship between \\(X\\) and \\(Y\\). The alternative hypothesis (\\(H_a\\)) states that the slope is non-zero, indicating a significant relationship. If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that there is a significant relationship between \\(X\\) and \\(Y\\). 31.2 Python Code Simple Linear Regression import pandas as pd import statsmodels.api as sm # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent and dependent variables X = df[&#39;sepal_length&#39;] y = df[&#39;sepal_width&#39;] # Add a constant (intercept) to the independent variable X = sm.add_constant(X) # Perform linear regression model = sm.OLS(y, X).fit() # Display results print(model.summary()) # Interpretation if model.pvalues[1] &lt; 0.05: print(&quot;Reject H0: There is a significant linear relationship between sepal length and sepal width.&quot;) else: print(&quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot;) OLS Regression Results ============================================================================== Dep. Variable: sepal_width R-squared: 0.014 Model: OLS Adj. R-squared: 0.007 Method: Least Squares F-statistic: 2.074 Date: Tue, 18 Mar 2025 Prob (F-statistic): 0.152 Time: 14:56:34 Log-Likelihood: -86.732 No. Observations: 150 AIC: 177.5 Df Residuals: 148 BIC: 183.5 Df Model: 1 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const 3.4189 0.254 13.484 0.000 2.918 3.920 sepal_length -0.0619 0.043 -1.440 0.152 -0.147 0.023 ============================================================================== Omnibus: 2.474 Durbin-Watson: 1.263 Prob(Omnibus): 0.290 Jarque-Bera (JB): 1.994 Skew: 0.243 Prob(JB): 0.369 Kurtosis: 3.288 Cond. No. 43.4 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Fail to reject H0: No significant linear relationship between sepal length and sepal width. 31.3 R Code Simple Linear Regression # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Select independent and dependent variables X &lt;- df$sepal_length y &lt;- df$sepal_width # Perform linear regression model &lt;- lm(y ~ X) # Display results summary(model) Call: lm(formula = y ~ X) Residuals: Min 1Q Median 3Q Max -1.1095 -0.2454 -0.0167 0.2763 1.3338 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.41895 0.25356 13.48 &lt;2e-16 *** X -0.06188 0.04297 -1.44 0.152 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4343 on 148 degrees of freedom Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 # Interpretation if (summary(model)$coefficients[2,4] &lt; 0.05) { print(&quot;Reject H0: There is a significant linear relationship between sepal length and sepal width.&quot;) } else { print(&quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot;) } [1] &quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot; "],["how-to-perform-multiple-linear-regression-in-python-and-r.html", "32 How to Perform Multiple Linear Regression in Python and R? 32.1 Explanation 32.2 Python Code 32.3 R Code", " 32 How to Perform Multiple Linear Regression in Python and R? 32.1 Explanation Multiple Linear Regression is an extension of simple linear regression that models the relationship between a dependent variable (\\(Y\\)) and two or more independent variables (\\(X_1, X_2, \\dots, X_n\\)). The equation for multiple linear regression is: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon \\] Where: - \\(Y\\) is the dependent variable, - \\(X_1, X_2, \\dots, X_n\\) are the independent variables, - \\(\\beta_0\\) is the intercept, - \\(\\beta_1, \\beta_2, \\dots, \\beta_n\\) are the coefficients (slopes) for the predictors, - \\(\\epsilon\\) is the error term. The null hypothesis (\\(H_0\\)) assumes that all regression coefficients are zero, meaning no relationship exists between the predictors and the dependent variable. The alternative hypothesis (\\(H_a\\)) suggests that at least one coefficient is non-zero, indicating a significant relationship. If the p-value for a coefficient is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis for that predictor and conclude that it significantly affects \\(Y\\). 32.2 Python Code import pandas as pd import statsmodels.api as sm # Load dataset (you can use any dataset here) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Predictors # Add a constant (intercept) to the model X = sm.add_constant(X) # Dependent variable (response) y = df[&#39;sepal_length&#39;] # Example of using sepal length as the dependent variable # Fit the model model = sm.OLS(y, X).fit() # Display the results print(model.summary()) OLS Regression Results ============================================================================== Dep. Variable: sepal_length R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 9.587e+29 Date: Tue, 18 Mar 2025 Prob (F-statistic): 0.00 Time: 14:56:34 Log-Likelihood: 4724.3 No. Observations: 150 AIC: -9439. Df Residuals: 145 BIC: -9424. Df Model: 4 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const -9.104e-15 4.83e-15 -1.887 0.061 -1.86e-14 4.34e-16 sepal_length 1.0000 1.36e-15 7.36e+14 0.000 1.000 1.000 sepal_width 1.11e-15 1.41e-15 0.790 0.431 -1.67e-15 3.89e-15 petal_length 2.776e-16 1.34e-15 0.207 0.836 -2.37e-15 2.92e-15 petal_width -1.554e-15 2.23e-15 -0.698 0.486 -5.95e-15 2.84e-15 ============================================================================== Omnibus: 5.539 Durbin-Watson: 0.030 Prob(Omnibus): 0.063 Jarque-Bera (JB): 3.049 Skew: 0.099 Prob(JB): 0.218 Kurtosis: 2.330 Cond. No. 91.9 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 32.3 R Code Multiple Linear Regression # Check if &#39;caret&#39; is installed, if not, install it if (!require(caret)) { # Set CRAN mirror options(repos = c(CRAN = &quot;https://cran.rstudio.com/&quot;)) install.packages(&quot;caret&quot;) library(caret) } # Load dataset (use any dataset available) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$sepal_length, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the linear regression model model &lt;- train(sepal_length ~ sepal_width + petal_length + petal_width, data = trainData, method = &quot;lm&quot;) # Display the model details print(model) Linear Regression 107 samples 3 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 107, 107, 107, 107, 107, 107, ... Resampling results: RMSE Rsquared MAE 0.3188897 0.8743628 0.26081 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # Predict on the test set predictions &lt;- predict(model, testData) # Display predictions and actual values predictions 1 2 7 15 18 23 25 27 5.024336 4.670085 4.878372 5.219239 4.949222 4.776489 5.351858 4.962606 30 31 33 36 42 49 52 56 4.971134 4.900284 5.604227 4.652436 4.019344 5.245711 6.305208 6.172036 59 67 68 74 75 77 82 85 6.322560 6.163508 6.007830 6.406499 6.083537 6.335945 5.476581 6.163508 87 88 91 93 99 100 105 108 6.393707 5.738110 6.025775 5.707077 4.914596 5.853338 6.673476 7.301455 111 116 124 126 127 128 133 142 6.407684 6.341690 6.044312 7.274983 6.035488 6.256863 6.372426 6.111491 144 146 149 6.819737 6.120315 6.563065 testData$sepal_length [1] 5.1 4.9 4.6 5.8 5.1 4.6 4.8 5.0 4.7 4.8 5.2 5.0 4.5 5.3 6.4 5.7 6.6 5.6 5.8 [20] 6.1 6.4 6.8 5.5 5.4 6.7 6.3 5.5 5.8 5.1 5.7 6.5 7.3 6.5 6.4 6.3 7.2 6.2 6.1 [39] 6.4 6.9 6.8 6.7 6.2 # Evaluate the model using RMSE (Root Mean Squared Error) rmse &lt;- sqrt(mean((predictions - testData$sepal_length)^2)) cat(&quot;RMSE: &quot;, rmse) RMSE: 0.3445695 "],["how-to-perform-logistic-regression-in-python-and-r.html", "33 How to Perform Logistic Regression in Python and R? 33.1 Explanation 33.2 Python Code 33.3 R Code", " 33 How to Perform Logistic Regression in Python and R? 33.1 Explanation Logistic Regression is a statistical method used for binary classification. It models the relationship between a dependent variable (binary outcome, e.g., 0 or 1) and one or more independent variables. The logistic regression equation is: \\[ \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n \\] Where: - \\(p\\) is the probability of the dependent event occurring (i.e., the probability that the output is 1), - \\(X_1, X_2, \\dots, X_n\\) are the independent variables (predictors), - \\(\\beta_0\\) is the intercept, - \\(\\beta_1, \\beta_2, \\dots, \\beta_n\\) are the coefficients (slopes) for the predictors. The model estimates the odds of the event occurring by taking the log of the odds ratio. The predicted probabilities are obtained using the logistic function, which is: \\[ p = \\frac{1}{1 + e^{-z}} \\] Where \\(z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\\). If the p-value of a coefficient is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that the predictor significantly influences the outcome. 33.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any binary classification dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Predictors # Create a binary dependent variable (response) df[&#39;is_setosa&#39;] = (df[&#39;species&#39;] == &#39;setosa&#39;).astype(int) # Convert &#39;setosa&#39; to 1, others to 0 y = df[&#39;is_setosa&#39;] # Target variable # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the logistic regression model model = LogisticRegression() model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[26 0] [ 0 19]] 33.3 R Code library(caret) # Load dataset (use any binary classification dataset) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create a binary dependent variable (response) df$is_setosa &lt;- ifelse(df$species == &#39;setosa&#39;, 1, 0) # Convert &#39;setosa&#39; to 1, others to 0 # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$is_setosa, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the logistic regression model model &lt;- train(is_setosa ~ sepal_length + sepal_width + petal_length + petal_width, data = trainData, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10)) # Display the model details print(model) # Predict the probabilities using the logistic regression model pred_probs &lt;- predict(model, testData, type = &quot;prob&quot;)[, 2] # Get the probability of class 1 # Convert probabilities to class predictions (0 or 1) pred_class &lt;- ifelse(pred_probs &gt; 0.5, 1, 0) # Evaluate the model using confusion matrix conf_matrix &lt;- confusionMatrix(factor(pred_class), factor(testData$is_setosa)) # Print confusion matrix print(conf_matrix) "],["how-to-perform-decision-tree-classification-in-python-and-r.html", "34 How to Perform Decision Tree Classification in Python and R? 34.1 Explanation 34.2 Python Code 34.3 R Code", " 34 How to Perform Decision Tree Classification in Python and R? 34.1 Explanation Decision Trees are a non-linear model used for both classification and regression. They split the dataset into subsets based on the most significant features, creating a tree-like structure where each internal node represents a feature or attribute, and each leaf node represents a decision or class label. The decision tree algorithm works by selecting the feature that best splits the data at each node based on certain criteria (e.g., Gini impurity, entropy for classification, and variance reduction for regression). Classification Trees: Used when the target variable is categorical. For example, in a binary classification problem, the decision tree can predict class 0 or class 1 based on the input features. Regression Trees: Used when the target variable is continuous. The tree is built recursively by selecting the best splits at each node and stopping when a stopping criterion (e.g., maximum depth, minimum samples per leaf) is met. 34.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Features # Select target variable (species) y = df[&#39;species&#39;] # Target # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the decision tree classifier model = DecisionTreeClassifier(random_state=42) model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[19 0 0] [ 0 13 0] [ 0 0 13]] 34.3 R Code # Load required library library(caret) # Load dataset (use any dataset) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$species, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the model using decision tree classifier model &lt;- train(species ~ sepal_length + sepal_width + petal_length + petal_width, data = trainData, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10)) # Display the model details print(model) # Predict using the decision tree model pred &lt;- predict(model, testData) # Evaluate the model using confusion matrix confusionMatrix(pred, testData$species) "],["how-to-perform-random-forest-classification-in-python-and-r.html", "35 How to Perform Random Forest Classification in Python and R? 35.1 Explanation 35.2 Python Code 35.3 R Code", " 35 How to Perform Random Forest Classification in Python and R? 35.1 Explanation Random Forest is an ensemble learning method that combines multiple decision trees to improve classification accuracy. Instead of relying on a single decision tree, random forest aggregates the predictions of many trees, reducing overfitting and improving generalization. Bagging (Bootstrap Aggregating): Random Forest uses bagging, which means training multiple models (decision trees) on random subsets of the data. Each tree is trained on a different random sample, and the final prediction is made by averaging (for regression) or majority voting (for classification) of all the trees’ predictions. Random Feature Selection: At each split in the decision tree, a random subset of features is selected, ensuring that trees are diverse and reducing the correlation between them. The main advantages of random forest are its robustness, the ability to handle a large number of features, and its capacity to deal with overfitting. 35.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Features # Select target variable (species) y = df[&#39;species&#39;] # Target # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the random forest classifier model = RandomForestClassifier(n_estimators=100, random_state=42) model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[19 0 0] [ 0 13 0] [ 0 0 13]] 35.3 R Code # Load required library library(caret) # Load dataset (use any dataset) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$species, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the model using Random Forest model &lt;- train(species ~ sepal_length + sepal_width + petal_length + petal_width, data = trainData, method = &quot;rf&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10)) # Display the model details print(model) # Predict using the random forest model pred &lt;- predict(model, testData) # Evaluate the model using confusion matrix confusionMatrix(pred, testData$species) "],["how-to-perform-support-vector-machine-svm-classification-in-python-and-r.html", "36 How to Perform Support Vector Machine (SVM) Classification in Python and R? 36.1 Explanation 36.2 Python Code 36.3 R Code", " 36 How to Perform Support Vector Machine (SVM) Classification in Python and R? 36.1 Explanation Support Vector Machine (SVM) is a powerful supervised learning algorithm that can be used for both classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes in a high-dimensional space. Linear SVM: Finds a straight line or hyperplane that divides the classes. Non-linear SVM: Uses kernel functions (like Radial Basis Function (RBF)) to transform the data into higher dimensions to make it linearly separable. The main objective of SVM is to maximize the margin between the two classes. The margin is the distance between the hyperplane and the closest data points from either class, known as support vectors. The SVM classifier works well for both linear and non-linear classification problems. 36.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Features # Select target variable (species) y = df[&#39;species&#39;] # Target # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the SVM classifier model = SVC(kernel=&#39;linear&#39;, random_state=42) model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[19 0 0] [ 0 13 0] [ 0 0 13]] 36.3 R Code # Load required library library(caret) # Load dataset (use any dataset) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$species, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the model using SVM with a linear kernel model &lt;- train(species ~ sepal_length + sepal_width + petal_length + petal_width, data = trainData, method = &quot;svmLinear&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10)) # Display the model details print(model) # Predict using the SVM model pred &lt;- predict(model, testData) # Evaluate the model using confusion matrix confusionMatrix(pred, testData$species) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
