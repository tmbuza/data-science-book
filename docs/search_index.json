[["index.html", "Learn, Compare, and Master Data Science in Python &amp; R Introduction Preface Motive", " Learn, Compare, and Master Data Science in Python &amp; R Introduction Preface This bilingual guide is designed to help beginners learn data science using both Python and R. The content is presented side by side in both languages, providing a clear comparison and making it easier to understand key data science concepts. Covering essential topics such as data loading, cleaning, visualization, and basic machine learning techniques, this step-by-step guide ensures that learners build strong foundational skills. The book follows a Q&amp;A format, where each section serves as both an instructional guide and a practical exercise. Each Q&amp;A introduces a data science concept, explains its significance, and provides a hands-on implementation in Python and R. This structure ensures that learners not only understand theoretical concepts but also apply them in real-world scenarios. This is a beginner-friendly guide, though some sections may introduce slightly more advanced topics that overlap with intermediate-level concepts. These inclusions are intentional, providing a smooth transition to the next level of learning. Readers who grasp these foundational concepts will find it easier to progress to the intermediate version of this guide. Each chapter is structured to facilitate easy navigation, and chapters are displayed in the left navigation bar. Simply click the hyperlinked text to jump to specific sections. Motive In recent years, artificial intelligence (AI) has made remarkable strides, with tools like ChatGPT and automated coding assistants becoming widely available. These technologies can accelerate coding, automate data analysis, and even generate complex models. However, a well-structured, human-curated approach remains essential for ensuring accuracy, interpretability, and ethical considerations in data science. Python and R are two of the most popular programming languages in data science, each offering unique strengths: Python is known for its simplicity, versatility, and powerful libraries like pandas, numpy, matplotlib, and scikit-learn, which make data manipulation, visualization, and machine learning accessible. It integrates well with various technologies, making it ideal for both beginners and experienced data scientists. R, designed specifically for data analysis and visualization, excels at statistical modeling and high-quality graphical representations with libraries like ggplot2, dplyr, and caret. It is widely used in academic research and industries that require robust statistical analysis. This guide presents Python and R solutions side by side, allowing learners to appreciate the strengths of each language. The Q&amp;A format ensures a smooth learning experience by combining structured explanations with hands-on coding exercises. Each section starts with a fundamental question, followed by clear explanations and implementation in both Python and R. The beginner version of this book lays the groundwork by covering essential tasks such as setting up your development environment, organizing your project directory, and loading your first dataset. As learners progress, they will encounter some slightly more advanced concepts that act as a stepping stone to the intermediate version of this guide. By working through this guide, learners will build a solid foundation in data science, preparing them for real-world applications. AI-generated insights should always be complemented by human expertise in interpreting, refining, and validating results. Updated on Mar 20, 2025 "],["how-to-set-up-your-development-environment.html", "1 How to Set Up Your Development Environment 1.1 Explanation 1.2 Install Python 1.3 Install R 1.4 Install VSCode 1.5 How to Navigate This Guide 1.6 Best Practices for Trying Python &amp; R Side by Side Transition to Data Science Q&amp;A", " 1 How to Set Up Your Development Environment 1.1 Explanation Setting up the right environment is crucial for coding in Python and R. Follow these steps to ensure you‚Äôre ready for the journey ahead. Before diving into the Q&amp;A sections, it‚Äôs important to ensure your development environment is set up properly. This section will guide you through the steps needed to install and configure the necessary tools to get started with both Python and R for data science. Once your environment is ready, you‚Äôll be able to follow the Q&amp;A sections effectively. 1.2 Install Python Visit the official Python website to download and install the latest version of Python. During installation, make sure to check the box to add Python to your PATH. 1.3 Install R Visit the official R Project website to download and install the latest version of R. Follow the installation instructions based on your operating system. 1.4 Install VSCode Visual Studio Code (VSCode) is a free and powerful code editor that supports both Python and R. It offers an interactive environment for writing, running, and debugging code efficiently. Download and install Visual Studio Code (VSCode) from the official VSCode website. After installation, open VSCode and install the necessary extensions for Python and R. Installing Extensions in VSCode For Python: Open VSCode. Press Ctrl + Shift + X (Windows/Linux) or Cmd + Shift + X (Mac) to open the Extensions Marketplace. Search for Python (developed by Microsoft) and click Install. This extension provides: IntelliSense (code completion, function suggestions, and real-time error checking). Debugging support. Jupyter Notebook integration. Formatting and linting tools. For R: In the Extensions Marketplace, search for R and click Install. This extension provides: IntelliSense for R functions and datasets. An integrated R terminal for running scripts. Debugging support. IntelliSense is a code assistance feature that helps you write code faster and with fewer errors. It provides autocomplete suggestions, function hints, and real-time error checking, making coding in both Python and R more efficient. Verify Installation After installing VSCode and the necessary extensions: Open VSCode and ensure that the extensions are enabled. Open a Python or R script to ensure that syntax highlighting and IntelliSense work correctly. If you‚Äôre using R, ensure R is installed on your system so that the R extension can detect it. 1.5 How to Navigate This Guide This guide follows a Q&amp;A format, where each section presents a question, an explanation, and solutions in both Python and R. This structure allows you to compare approaches side by side, reinforcing your understanding of data science concepts. Q&amp;A Format Each topic is structured as follows: Question: A clear problem statement. Explanation: A breakdown of the concept with key insights. Python Code: Solution using Python. R Code: Solution using R. 1.6 Best Practices for Trying Python &amp; R Side by Side To get the most out of this guide, follow these best practices: 1. Run Python and R Solutions Separately Each language has its own syntax and libraries, so run the solutions independently in Jupyter Notebook (Python) and RStudio (R). 2. Experiment with Code Modify the examples, change values, and try new functions to deepen your understanding. 3. Compare Outputs Observe how results differ between Python and R, especially in data visualization and statistical analysis. 4. Use the Same Dataset Ensure you‚Äôre using the same dataset in both Python and R to maintain consistency when comparing results. By following this approach, you‚Äôll develop a strong foundation in both languages and gain confidence in handling data science tasks. Transition to Data Science Q&amp;A Now that you‚Äôve set up your development environment and understood how to navigate this guide, you are ready to dive into the first Q&amp;A section. This section will introduce the first practical task in data science: How to Install Basic Libraries for Python and R. Installing essential libraries is the first step in using Python and R for data science. Let‚Äôs get started! üöÄ "],["how-to-install-basic-libaries-for-python-and-r.html", "2 How to Install Basic Libaries for Python and R 2.1 Explanation 2.2 Python Code 2.3 R Code", " 2 How to Install Basic Libaries for Python and R 2.1 Explanation Before you can load datasets in Python and R, you need to install the necessary libraries. Here‚Äôs how you can install the basic libraries required for this guide: 2.2 Python Code In your terminal, run the following command to install the necessary libraries for data manipulation, visualization, and machine learning: pip install pandas matplotlib scikit-learn 2.3 R Code In R, install the following packages to help with data manipulation and visualization: if (!require(tidyverse)) {install.packages(&#39;tidyverse&#39;)} library(tidyverse) if (!require(GGally)) {install.packages(&#39;GGally&#39;)} library(GGally) if (!require(caret)) {install.packages(&#39;caret&#39;)} library(caret) The tidyverse R package contains a collection of essential packages for data science, including: ggplot2 for data visualization dplyr for data manipulation tidyr for data tidying readr for reading and writing data files purrr for functional programming tibble for an improved data frame format stringr for string manipulation forcats for categorical variable handling The caret package (short for Classification and Regression Training) is widely used for machine learning in R. It provides a unified interface for training and evaluating models, making it easier to apply machine learning techniques. These libraries and packages will ensure you have the tools you need to get started with data analysis and visualization in both languages. "],["set-project-dir.html", "3 How to Create a Project Directory in Python and R 3.1 Explanation 3.2 Python Code 3.3 R Code", " 3 How to Create a Project Directory in Python and R 3.1 Explanation A well-organized project directory is key to efficient data science work. In this section, you will set up a project directory with separate folders for your datasets and scripts. Now that you have your environment set up, it‚Äôs time to create your project directory. This will help keep your files organized as you progress through the guide. Create a new folder for your project, such as beginner-data-science. Inside this folder, create the following subfolders: data: This folder will store your datasets. scripts: Store Python or R scripts here. images: Use this folder for images related to the project. Example Structure: beginner-data-science/ ‚îú‚îÄ‚îÄ data/ ‚îú‚îÄ‚îÄ scripts/ ‚îî‚îÄ‚îÄ images/ 3.2 Python Code import os # Define project structure project_dir = &#39;./&#39; data_dir = os.path.join(project_dir, &#39;data&#39;) scripts_dir = os.path.join(project_dir, &#39;scripts&#39;) images_dir = os.path.join(project_dir, &#39;images&#39;) # Create directories os.makedirs(data_dir, exist_ok=True) os.makedirs(scripts_dir, exist_ok=True) os.makedirs(images_dir, exist_ok=True) print(f&quot;Project directory structure created at {project_dir}&quot;) Project directory structure created at ./ 3.3 R Code # Define project structure project_dir &lt;- &quot;./&quot; data_dir &lt;- file.path(project_dir, &quot;data&quot;) scripts_dir &lt;- file.path(project_dir, &quot;scripts&quot;) images_dir &lt;- file.path(project_dir, &quot;images&quot;) # Create directories dir.create(data_dir, showWarnings = FALSE) dir.create(scripts_dir, showWarnings = FALSE) dir.create(images_dir, showWarnings = FALSE) cat(&quot;Project directory structure created at&quot;, project_dir, &quot;\\n&quot;) Project directory structure created at ./ "],["how-to-save-a-dataset-in-python-and-r.html", "4 How to Save a Dataset in Python and R 4.1 Explanation 4.2 Python Code 4.3 R Code", " 4 How to Save a Dataset in Python and R 4.1 Explanation Saving datasets is essential for storing processed data, sharing results, and reusing data in later analysis. In Python, we commonly use pandas to save datasets in CSV format. In R, readr::write_csv() and write.csv() are common functions for saving datasets. You will need to: Iris dataset is available from this link. Also comes with some python libraries such as sklearn.datasets in python and data() in R. Note! When working with datasets in both Python and R, it‚Äôs essential to save them in a structured format. However, the column names in the Iris dataset differ slightly between Python and R: Python (pandas version) uses: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), species R (datasets::iris version) uses: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species To maintain consistency, we will save two separate versions: iris-py.csv (from Python) iris-r.csv (from R) This ensures that each dataset retains its original structure before standardization. 4.2 Python Code import pandas as pd from sklearn.datasets import load_iris # Load the full iris dataset iris = load_iris(as_frame=True) df = iris.frame # Add species names df[&quot;species&quot;] = df[&quot;target&quot;].map({0: &quot;setosa&quot;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) # Save the dataset as &quot;iris-py.csv&quot; df.to_csv(&quot;data/iris_py.csv&quot;, index=False) print(&quot;Dataset saved as &#39;data/iris_py.csv&#39;&quot;) Dataset saved as &#39;data/iris_py.csv&#39; # Add species names df[&quot;species&quot;] = df[&quot;target&quot;].map({0: &quot;setosa&quot;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) # Save dataset with all columns df.to_csv(&quot;data/iris_py.csv&quot;, index=False) print(&quot;Dataset saved as &#39;data/iris_py.csv&#39;&quot;) Dataset saved as &#39;data/iris_py.csv&#39; 4.3 R Code # Load necessary libraries library(readr) # Load the full iris dataset df &lt;- datasets::iris # Save the dataset with species included using write_csv from readr write_csv(df, &quot;data/iris_r.csv&quot;) # Confirmation message print(&quot;Dataset saved as &#39;data/iris_r.csv&#39;&quot;) [1] &quot;Dataset saved as &#39;data/iris_r.csv&#39;&quot; "],["how-to-rename-column-names-in-python-and-r.html", "5 How to Rename Column Names in Python and R? 5.1 Explanation 5.2 Python Code 5.3 R Code", " 5 How to Rename Column Names in Python and R? 5.1 Explanation Since the column names differ between Python and R versions of the dataset, we will standardize them to ensure consistency. The renamed column names will be: sepal_length sepal_width petal_length petal_width species This makes it easier to work with the dataset across different tools and languages. After Renaming, we will save the final dataset as iris.csv. 5.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris_py.csv&quot;) # Rename columns df.rename(columns={&#39;sepal length (cm)&#39;: &#39;sepal_length&#39;, &#39;sepal width (cm)&#39;: &#39;sepal_width&#39;, &#39;petal length (cm)&#39;: &#39;petal_length&#39;, &#39;petal width (cm)&#39;: &#39;petal_width&#39;, &#39;species&#39;: &#39;species&#39;}, inplace=True) # Save the standardized dataset df.to_csv(&quot;data/iris.csv&quot;, index=False) print(&quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot;) Renamed dataset saved as &#39;data/iris.csv&#39; 5.3 R Code library(readr) library(dplyr) # Load dataset df &lt;- read_csv(&quot;data/iris_r.csv&quot;) # Rename columns df &lt;- df %&gt;% rename(sepal_length = Sepal.Length, sepal_width = Sepal.Width, petal_length = Petal.Length, petal_width = Petal.Width, species = Species) # Save the standardized dataset write_csv(df, &quot;data/iris.csv&quot;) print(&quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot;) [1] &quot;Renamed dataset saved as &#39;data/iris.csv&#39;&quot; "],["how-to-load-a-dataset-in-python-and-r.html", "6 How to load a dataset in Python and R? 6.1 Explanation 6.2 Python Code 6.3 R Code", " 6 How to load a dataset in Python and R? 6.1 Explanation Loading a dataset is one of the first steps in any data analysis project. In this case, we‚Äôll load the Iris dataset, a popular dataset for beginner data science projects, in both Python and R. The dataset has been saved as iris.csv in your data folder. We will use pandas in Python and readr in R to load the dataset into a dataframe. In Python we will use the pandas library, which is a powerful tool for data manipulation and analysis. The read_csv() function in pandas will allow us to read the iris.csv file into a dataframe. In R we will use the readr package, which provides modern and faster alternatives to base R functions. The read_csv() function in readr is similar to pandas in Python and offers a streamlined approach for loading CSV files. 6.2 Python Code import pandas as pd # Load the dataset iris = pd.read_csv(&#39;data/iris.csv&#39;) # Show the first few rows print(iris.head()) sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 6.3 R Code # Load necessary library library(readr) # Load dataset from CSV file df &lt;- read_csv(&quot;data/iris.csv&quot;) # Display the first few rows head(df) # A tibble: 6 √ó 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa "],["how-to-explore-a-dataset-in-python-and-r.html", "7 How to Explore a Dataset in Python and R? 7.1 Explanation 7.2 Python Code 7.3 R Code", " 7 How to Explore a Dataset in Python and R? 7.1 Explanation After loading a dataset, it is important to explore its structure, summary statistics, and key properties before performing any analysis. This helps in understanding the data and identifying potential issues such as missing values or outliers. In this section, we will: View the structure of the dataset. Get summary statistics. Check for missing values. 7.2 Python Code import pandas as pd # Load the dataset iris = pd.read_csv(&#39;data/iris.csv&#39;) # Display basic information about the dataset print(&quot;Dataset Information:&quot;) print(iris.info()) # Show summary statistics print(&quot;\\nSummary Statistics:&quot;) print(iris.describe()) # Check for missing values print(&quot;\\nMissing Values:&quot;) print(iris.isnull().sum()) Dataset Information: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 sepal_length 150 non-null float64 1 sepal_width 150 non-null float64 2 petal_length 150 non-null float64 3 petal_width 150 non-null float64 4 target 150 non-null int64 5 species 150 non-null object dtypes: float64(4), int64(1), object(1) memory usage: 7.2+ KB None Summary Statistics: sepal_length sepal_width petal_length petal_width target count 150.000000 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 1.000000 std 0.828066 0.435866 1.765298 0.762238 0.819232 min 4.300000 2.000000 1.000000 0.100000 0.000000 25% 5.100000 2.800000 1.600000 0.300000 0.000000 50% 5.800000 3.000000 4.350000 1.300000 1.000000 75% 6.400000 3.300000 5.100000 1.800000 2.000000 max 7.900000 4.400000 6.900000 2.500000 2.000000 Missing Values: sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 species 0 dtype: int64 7.3 R Code # Load necessary library library(readr) library(dplyr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Display the structure of the dataset cat(&quot;Dataset Structure:\\n&quot;) Dataset Structure: str(df) spc_tbl_ [150 √ó 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ sepal_length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ petal_length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ species : chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... - attr(*, &quot;spec&quot;)= .. cols( .. sepal_length = col_double(), .. sepal_width = col_double(), .. petal_length = col_double(), .. petal_width = col_double(), .. species = col_character() .. ) - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Show summary statistics cat(&quot;\\nSummary Statistics:\\n&quot;) Summary Statistics: summary(df) sepal_length sepal_width petal_length petal_width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 species Length:150 Class :character Mode :character # Check for missing values cat(&quot;\\nMissing Values:\\n&quot;) Missing Values: colSums(is.na(df)) sepal_length sepal_width petal_length petal_width species 0 0 0 0 0 "],["how-to-handle-missing-data-in-python-and-r.html", "8 How to Handle Missing Data in Python and R? 8.1 Explanation 8.2 Python Code 8.3 R Code", " 8 How to Handle Missing Data in Python and R? 8.1 Explanation Handling missing data is one of the most important steps in data cleaning. In this section, we‚Äôll explore how to handle missing values in a dataset. There are several strategies for handling missing data, such as: Removing missing values Imputing missing values (filling them with a specific value or a calculated statistic) In this guide, we will focus on removing missing values, though you can also explore imputation depending on your data and goals. 8.2 Python Code In Python, we can use pandas to detect and handle missing data. The isna() and dropna() functions are commonly used for this task. import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) df # Check for missing values print(df.isna().sum()) # Remove rows with missing data df_cleaned = df.dropna() # Check the cleaned data print(df_cleaned.head()) sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 species 0 dtype: int64 sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 8.3 R Code In R, we can use the is.na() function to detect missing values, and the na.omit() function to remove them. library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Check for missing values missing_values &lt;- colSums(is.na(df)) print(missing_values) sepal_length sepal_width petal_length petal_width species 0 0 0 0 0 # Remove rows with missing data df_cleaned &lt;- na.omit(df) # Check the cleaned data head(df_cleaned) # A tibble: 6 √ó 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa Handling missing data properly ensures that your analysis is accurate and that missing values do not introduce bias into your model or analysis. "],["how-to-filter-data-in-python-and-r.html", "9 How to Filter Data in Python and R? 9.1 Explanation 9.2 Python Code 9.3 R Code", " 9 How to Filter Data in Python and R? 9.1 Explanation Filtering data is a common task in data science. It allows you to select specific rows based on conditions or criteria, helping you focus on the subset of data that is most relevant to your analysis. In this section, we will learn how to filter data using conditions in both Python and R. We‚Äôll demonstrate filtering rows based on specific column values. 9.2 Python Code In Python, we use pandas to filter data. The loc[] method is useful for selecting rows based on specific conditions. import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Filter rows where species is &#39;setosa&#39; setosa_df = df[df[&#39;species&#39;] == &#39;setosa&#39;] # Show the first few rows of the filtered data print(setosa_df.head()) sepal_length sepal_width petal_length petal_width target species 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa 9.3 R Code In R, we can use the dplyr package to filter rows based on specific conditions. The filter() function is used for this purpose. library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Filter rows where species is &#39;setosa&#39; setosa_df &lt;- df %&gt;% filter(species == &#39;setosa&#39;) # Show the first few rows of the filtered data head(setosa_df) # A tibble: 6 √ó 5 sepal_length sepal_width petal_length petal_width species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa Filtering data allows you to narrow down your dataset and focus on specific insights or subsets of interest. In real-world projects, filtering is often one of the first steps in analyzing a dataset. "],["how-to-group-data-in-python-and-r.html", "10 How to Group Data in Python and R? 10.1 Explanation 10.2 Python Code 10.3 R Code", " 10 How to Group Data in Python and R? 10.1 Explanation Grouping data is a common task when summarizing and analyzing datasets. In this section, we‚Äôll group the Iris dataset by the Species column and calculate summary statistics (e.g., mean) for each group. In Python, we use pandas for this task, while in R we use dplyr. Let‚Äôs explore how to group the data by species and calculate the mean of each numeric column. 10.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Group by &#39;Species&#39; and calculate the mean for each group grouped_df = df.groupby(&#39;species&#39;).mean() # Display the grouped data print(grouped_df) sepal_length sepal_width petal_length petal_width target species setosa 5.006 3.428 1.462 0.246 0.0 versicolor 5.936 2.770 4.260 1.326 1.0 virginica 6.588 2.974 5.552 2.026 2.0 10.3 R Code library(dplyr) library(readr) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Group by &#39;species&#39; and calculate the mean for each group grouped_df &lt;- df %&gt;% group_by(species) %&gt;% summarise(across(where(is.numeric), mean)) # Display the grouped data print(grouped_df) # A tibble: 3 √ó 5 species sepal_length sepal_width petal_length petal_width &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 setosa 5.01 3.43 1.46 0.246 2 versicolor 5.94 2.77 4.26 1.33 3 virginica 6.59 2.97 5.55 2.03 "],["how-to-aggregate-data-in-python-and-r.html", "11 How to Aggregate Data in Python and R? 11.1 Explanation 11.2 Python Code 11.3 R Code", " 11 How to Aggregate Data in Python and R? 11.1 Explanation Aggregation helps summarize datasets by computing statistics such as mean, median, count, or sum for different groups. This is useful when analyzing patterns within the dataset. For example, in the iris dataset, we can find the average sepal width per species to compare flower characteristics. 11.2 Python Code import pandas as pd # Load the dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Aggregate: Calculate mean sepal width per species agg_df = df.groupby(&quot;species&quot;)[&quot;sepal_width&quot;].mean().reset_index() # Save the aggregated data agg_df.to_csv(&quot;data/iris_aggregated_py.csv&quot;, index=False) # Display the result print(agg_df) species sepal_width 0 setosa 3.428 1 versicolor 2.770 2 virginica 2.974 11.3 R Code # Load necessary libraries library(readr) library(dplyr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Aggregate: Calculate mean sepal width per species agg_df &lt;- df %&gt;% group_by(species) %&gt;% summarise(mean_sepal_width = mean(sepal_width, na.rm = TRUE)) # Save the aggregated data write_csv(agg_df, &quot;data/iris_aggregated_r.csv&quot;) # Display the result print(agg_df) # A tibble: 3 √ó 2 species mean_sepal_width &lt;chr&gt; &lt;dbl&gt; 1 setosa 3.43 2 versicolor 2.77 3 virginica 2.97 "],["how-to-split-a-dataset-in-python-and-r.html", "12 How to Split a Dataset in Python and R? 12.1 Explanation 12.2 Python Code 12.3 R Code", " 12 How to Split a Dataset in Python and R? 12.1 Explanation Splitting a dataset into multiple parts is useful when you want to work with subsets of data. In this case, we will split the iris dataset into two parts: x - iris_part1.csv: Contains columns sepal_length, sepal_width, and species. - iris_part2.csv: Contains columns petal_length, petal_width, and species. These parts will later be merged based on the species column. 12.2 Python Code import pandas as pd # Load the iris dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Split into two parts part1 = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;species&#39;]] part2 = df[[&#39;petal_length&#39;, &#39;petal_width&#39;, &#39;species&#39;]] # Save the parts as separate CSV files part1.to_csv(&quot;data/iris_part1.csv&quot;, index=False) part2.to_csv(&quot;data/iris_part2.csv&quot;, index=False) # Display a message to confirm print(&quot;Parts saved as iris_part1.csv and iris_part2.csv&quot;) Parts saved as iris_part1.csv and iris_part2.csv 12.3 R Code # Load necessary libraries library(readr) # Load the iris dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Split into two parts part1 &lt;- df[, c(&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;species&quot;)] part2 &lt;- df[, c(&quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;)] # Save the parts as separate CSV files write_csv(part1, &quot;data/iris_part1.csv&quot;) write_csv(part2, &quot;data/iris_part2.csv&quot;) # Display a message to confirm cat(&quot;Parts saved as iris_part1.csv and iris_part2.csv\\n&quot;) Parts saved as iris_part1.csv and iris_part2.csv Now that we have created the two parts, we can proceed to merge them using the steps outlined previously. "],["how-to-merge-datasets-in-python-and-r.html", "13 How to Merge Datasets in Python and R? 13.1 Explanation 13.2 Python Code 13.3 R Code", " 13 How to Merge Datasets in Python and R? 13.1 Explanation Merging datasets is a common task when working with multiple data sources. In the iris dataset, we may want to combine different subsets of data based on a common column, such as the species. In this example, we assume there are two datasets: iris_part1.csv (contains sepal_length, sepal_width, and species) iris_part2.csv (contains petal_length, petal_width, and species) We will merge them on the species column. 13.2 Python Code import pandas as pd # Load the two parts of the iris dataset (with renamed columns) part1 = pd.read_csv(&quot;data/iris_part1.csv&quot;) part2 = pd.read_csv(&quot;data/iris_part2.csv&quot;) # Merge the datasets based on the &#39;species&#39; column merged_df = pd.merge(part1, part2, on=&#39;species&#39;) # Save the merged dataset as a new CSV file merged_df.to_csv(&quot;data/iris_merged.csv&quot;, index=False) # Display the first few rows of the merged dataset print(merged_df.head()) sepal_length sepal_width species petal_length petal_width 0 5.1 3.5 setosa 1.4 0.2 1 5.1 3.5 setosa 1.4 0.2 2 5.1 3.5 setosa 1.3 0.2 3 5.1 3.5 setosa 1.5 0.2 4 5.1 3.5 setosa 1.4 0.2 13.3 R Code # Load necessary library library(readr) # Load the two parts of the iris dataset (with renamed columns) part1 &lt;- read_csv(&quot;data/iris_part1.csv&quot;) part2 &lt;- read_csv(&quot;data/iris_part2.csv&quot;) # Merge the datasets based on the &#39;species&#39; column merged_df &lt;- merge(part1, part2, by = &quot;species&quot;) # Save the merged dataset as a new CSV file write_csv(merged_df, &quot;data/iris_merged.csv&quot;) # Display the first few rows of the merged dataset head(merged_df) species sepal_length sepal_width petal_length petal_width 1 setosa 5.1 3.5 1.4 0.2 2 setosa 5.1 3.5 1.4 0.2 3 setosa 5.1 3.5 1.3 0.2 4 setosa 5.1 3.5 1.5 0.2 5 setosa 5.1 3.5 1.4 0.2 6 setosa 5.1 3.5 1.7 0.4 "],["how-to-create-a-bar-plot-in-python-and-r.html", "14 How to Create a Bar Plot in Python and R? 14.1 Explanation 14.2 Python Code 14.3 R Code", " 14 How to Create a Bar Plot in Python and R? 14.1 Explanation A bar plot is used to visualize categorical data with rectangular bars representing the frequency or value of categories. 14.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a bar plot sns.countplot(x=&quot;species&quot;, data=df) plt.title(&quot;Bar Plot of Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Count&quot;) plt.show() 14.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a bar plot with fill color based on species ggplot(df, aes(x=species, fill=species)) + geom_bar() + theme_minimal() + ggtitle(&quot;Bar Plot of Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Count&quot;) "],["how-to-create-a-histogram-in-python-and-r.html", "15 How to Create a Histogram in Python and R? 15.1 Explanation 15.2 Python Code 15.3 R Code", " 15 How to Create a Histogram in Python and R? 15.1 Explanation A histogram displays the distribution of a continuous variable by dividing the data into bins or intervals. The height of each bar represents the frequency of values within that bin. 15.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a histogram sns.histplot(df[&#39;sepal_length&#39;], kde=True, bins=10) plt.title(&quot;Histogram of Sepal Length&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() 15.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a histogram ggplot(df, aes(x=sepal_length)) + geom_histogram(bins=10, fill=&quot;skyblue&quot;, color=&quot;black&quot;) + theme_minimal() + ggtitle(&quot;Histogram of Sepal Length&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Frequency&quot;) "],["how-to-create-a-scatter-plot-in-python-and-r.html", "16 How to Create a Scatter Plot in Python and R? 16.1 Explanation 16.2 Python Code 16.3 R Code", " 16 How to Create a Scatter Plot in Python and R? 16.1 Explanation A scatter plot is used to show the relationship between two continuous variables. Each point represents an observation in the dataset. 16.2 Python Code import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) df[&#39;species&#39;] = df[&#39;species&#39;].astype(&#39;category&#39;) # Ensure categorical variable # Set a better background sns.set_style(&quot;whitegrid&quot;) # Create a scatter plot with improved coloring ax = sns.scatterplot(x=&#39;sepal_length&#39;, y=&#39;sepal_width&#39;, hue=&#39;species&#39;, data=df, palette=&#39;Set1&#39;) # Ensure legend is displayed plt.legend(title=&quot;Species&quot;) # Add labels and title plt.title(&quot;Scatter Plot of Sepal Length vs Sepal Width by Species&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Sepal Width&quot;) # Show the plot plt.show() 16.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a scatter plot colored by species ggplot(df, aes(x=sepal_length, y=sepal_width, color=species)) + geom_point() + theme_minimal() + ggtitle(&quot;Scatter Plot of Sepal Length vs Sepal Width by Species&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Sepal Width&quot;) "],["how-to-create-a-line-plot-in-python-and-r.html", "17 How to Create a Line Plot in Python and R? 17.1 Explanation 17.2 Python Code 17.3 R Code", " 17 How to Create a Line Plot in Python and R? 17.1 Explanation A line plot is used to display the relationship between two continuous variables, where data points are connected by lines to show trends over time or other continuous variables. 17.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a line plot sns.lineplot(x=&#39;sepal_length&#39;, y=&#39;sepal_width&#39;, data=df) plt.title(&quot;Line Plot of Sepal Length vs Sepal Width&quot;) plt.xlabel(&quot;Sepal Length&quot;) plt.ylabel(&quot;Sepal Width&quot;) plt.show() 17.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a line plot with a specific color ggplot(df, aes(x=sepal_length, y=sepal_width)) + geom_line(color=&quot;steelblue&quot;) + # Set the line color to steelblue theme_minimal() + ggtitle(&quot;Line Plot of Sepal Length vs Sepal Width&quot;) + xlab(&quot;Sepal Length&quot;) + ylab(&quot;Sepal Width&quot;) "],["how-to-create-a-box-plot-in-python-and-r.html", "18 How to Create a Box Plot in Python and R? 18.1 Explanation 18.2 Python Code 18.3 R Code", " 18 How to Create a Box Plot in Python and R? 18.1 Explanation A box plot (or box-and-whisker plot) is used to display the distribution of a continuous variable, highlighting the median, quartiles, and potential outliers. 18.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a box plot sns.boxplot(x=&#39;species&#39;, y=&#39;sepal_length&#39;, data=df) plt.title(&quot;Box Plot of Sepal Length by Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Sepal Length&quot;) plt.show() 18.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a box plot ggplot(df, aes(x=species, y=sepal_length, fill=species)) + geom_boxplot() + theme_minimal() + ggtitle(&quot;Box Plot of Sepal Length by Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Sepal Length&quot;) "],["how-to-create-a-heatmap-in-python-and-r.html", "19 How to Create a Heatmap in Python and R? 19.1 Explanation 19.2 Python Code 19.3 R Code", " 19 How to Create a Heatmap in Python and R? 19.1 Explanation A heatmap is used to display data in matrix form, where individual values are represented by colors. It‚Äôs often used to visualize correlation matrices or other tabular data. 19.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Select only numeric columns for correlation numeric_df = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]) # Create a correlation matrix corr = numeric_df.corr() # Create a heatmap with a specified range for color scale sns.heatmap(corr, annot=True, cmap=&quot;coolwarm&quot;, vmin=-1, vmax=1) plt.title(&quot;Heatmap of Iris Dataset Correlations&quot;) plt.show() 19.3 R Code # Import libraries library(readr) # For reading CSV library(corrplot) # For correlation matrix visualization library(dplyr) # For data manipulation # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Select only numeric columns for correlation numeric_df &lt;- df %&gt;% select(where(is.numeric)) # Create a correlation matrix corr &lt;- cor(numeric_df) # Create a heatmap using corrplot with adjusted x-axis label position corrplot(corr, method = &quot;color&quot;, col = colorRampPalette(c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;))(200), addCoef.col = &quot;black&quot;, number.cex = 0.7, title = &quot;Heatmap of Iris Dataset Correlations&quot;, mar = c(0, 0, 2, 0), # Adjust margin size to create space for the title las = 1) # Rotate x-axis labels for better spacing "],["how-to-create-a-pair-plot-in-python-and-r.html", "20 How to Create a Pair Plot in Python and R? 20.1 Explanation 20.2 Python Code 20.3 R Code", " 20 How to Create a Pair Plot in Python and R? 20.1 Explanation A pair plot is used to visualize pairwise relationships between multiple variables. It‚Äôs particularly useful for understanding the correlations between multiple continuous variables. 20.2 Python Code import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import warnings # Suppress warnings # Hide warnings warnings.filterwarnings(&quot;ignore&quot;) # Load dataset from CSV df = pd.read_csv(&quot;data/iris.csv&quot;) # Exclude the target column and plot pairplot sns.pairplot(df, hue=&quot;species&quot;) # Adjust layout to prevent overlapping plt.tight_layout() # Show the plot plt.show() 20.3 R Code # Ensure required packages are installed and loaded if (!require(GGally)) install.packages(&quot;GGally&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) if (!require(ggplot2)) install.packages(&quot;ggplot2&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) if (!require(readr)) install.packages(&quot;readr&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) library(GGally) library(ggplot2) library(readr) # Load dataset from CSV df &lt;- read_csv(&quot;data/iris.csv&quot;) # Plot pairplot excluding the target column but using it as hue ggpairs(df, aes(color = species)) + theme_minimal() "],["how-to-create-a-violin-plot-in-python-and-r.html", "21 How to Create a Violin Plot in Python and R? 21.1 Explanation 21.2 Python Code 21.3 R Code", " 21 How to Create a Violin Plot in Python and R? 21.1 Explanation A violin plot combines aspects of a box plot and a kernel density plot. It provides a detailed view of the distribution of a continuous variable for different categories. 21.2 Python Code # Import libraries import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a violin plot sns.violinplot(x=&#39;species&#39;, y=&#39;sepal_length&#39;, data=df) plt.title(&quot;Violin Plot of Sepal Length by Species&quot;) plt.xlabel(&quot;Species&quot;) plt.ylabel(&quot;Sepal Length&quot;) plt.show() 21.3 R Code # Load the necessary libraries library(ggplot2) # Load the dataset df &lt;- read.csv(&#39;data/iris.csv&#39;) # Create a violin plot with grouping by color ggplot(df, aes(x=species, y=sepal_length, fill=species)) + geom_violin() + theme_minimal() + ggtitle(&quot;Violin Plot of Sepal Length by Species&quot;) + xlab(&quot;Species&quot;) + ylab(&quot;Sepal Length&quot;) "],["what-is-hypothesis-testing.html", "22 What is Hypothesis Testing?* 22.1 Explanation", " 22 What is Hypothesis Testing?* 22.1 Explanation Hypothesis testing is a statistical method used to make inferences or draw conclusions about a population based on a sample. It involves formulating a hypothesis, collecting data, and using statistical tests to determine whether there is enough evidence to reject the hypothesis. 22.1.1 Key Terms in Hypothesis Testing: Null Hypothesis (H‚ÇÄ): Assumes no effect or no difference (e.g., ‚ÄúThere is no difference in average heights between two groups‚Äù). Alternative Hypothesis (H‚ÇÅ or Ha): The statement we aim to test (e.g., ‚ÄúThere is a difference in average heights between two groups‚Äù). Significance Level (Œ±): The probability threshold for rejecting H‚ÇÄ (commonly set at 0.05). p-value: The probability of observing the data if H‚ÇÄ is true. If p &lt; Œ±, we reject H‚ÇÄ. Type I Error: Incorrectly rejecting H‚ÇÄ (false positive). Type II Error: Failing to reject H‚ÇÄ when it is false (false negative). 22.1.2 Steps in Hypothesis Testing: Formulate H‚ÇÄ and H‚ÇÅ. Choose a significance level (Œ±). Select the appropriate statistical test. Compute the test statistic and p-value. Compare the p-value with Œ± and draw conclusions. 22.1.3 Common Hypothesis Tests: t-tests (Comparing means of one or two groups) ANOVA (Comparing means of multiple groups) Chi-Square Test (Testing independence in categorical data) Non-parametric tests (For non-normally distributed data) 22.1.4 Hypothesis Testing Q&amp;A Topics What is Hypothesis Testing? (Introduction) How to Perform a One-Sample t-test in Python and R? How to Perform a Two-Sample t-test in Python and R? How to Perform a Paired t-test in Python and R? How to Perform a One-Way ANOVA in Python and R? How to Perform a Two-Way ANOVA in Python and R? How to Perform a Repeated Measures ANOVA in Python and R? How to Perform a Chi-Square Test in Python and R? How to Perform a Mann-Whitney U Test in Python and R? (Non-parametric alternative to t-test) How to Perform a Kruskal-Wallis Test in Python and R? (Non-parametric alternative to One-Way ANOVA) How to Perform a Friedman Test in Python and R? (Non-parametric alternative to Repeated Measures ANOVA) How to Check Normality in Python and R? (Shapiro-Wilk, Kolmogorov-Smirnov tests) How to Perform a Levene‚Äôs Test in Python and R? (Checking homogeneity of variances) Now, let‚Äôs move to the first statistical test: "],["how-to-perform-a-one-sample-t-test-in-python-and-r-using-the-iris-dataset.html", "23 How to Perform a One-Sample t-test in Python and R using the iris dataset? 23.1 Explanation 23.2 Python Code 23.3 R Code", " 23 How to Perform a One-Sample t-test in Python and R using the iris dataset? 23.1 Explanation A One-Sample t-test compares the mean of a sample to a known or hypothesized population mean. In this case, we‚Äôll test whether the mean sepal_length of the iris dataset is significantly different from a hypothesized population mean of 5.8 cm. 23.1.1 Hypotheses H0 (Null Hypothesis): The mean sepal_length in the sample is equal to 5.8 cm. H1 (Alternative Hypothesis): The mean sepal_length in the sample is different from 5.8 cm. 23.1.2 Dataset: We will use the iris dataset with columns: sepal_length, sepal_width, petal_length, petal_width, and species, stored in data/iris.csv. 23.2 Python Code import pandas as pd import scipy.stats as stats # Load the iris dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Extract Sepal Length column sepal_length = df[&quot;sepal_length&quot;] # Hypothesized population mean pop_mean = 5.8 # Perform one-sample t-test t_stat, p_value = stats.ttest_1samp(sepal_length, pop_mean) # Display results print(f&quot;T-Statistic: {t_stat:.4f}&quot;) print(f&quot;P-Value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: The mean sepal length is significantly different from 5.8 cm.&quot;) else: print(&quot;Fail to reject the null hypothesis: No significant difference from 5.8 cm.&quot;) T-Statistic: 0.6409 P-Value: 0.5226 Fail to reject the null hypothesis: No significant difference from 5.8 cm. 23.3 R Code # Load necessary libraries library(readr) # Load the iris dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Extract Sepal Length column sepal_length &lt;- df$sepal_length # Hypothesized population mean pop_mean &lt;- 5.8 # Perform one-sample t-test t_test_result &lt;- t.test(sepal_length, mu = pop_mean) # Display results print(t_test_result) One Sample t-test data: sepal_length t = 0.64092, df = 149, p-value = 0.5226 alternative hypothesis: true mean is not equal to 5.8 95 percent confidence interval: 5.709732 5.976934 sample estimates: mean of x 5.843333 # Interpretation if (t_test_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: The mean sepal length is significantly different from 5.8 cm.&quot;) } else { print(&quot;Fail to reject the null hypothesis: No significant difference from 5.8 cm.&quot;) } [1] &quot;Fail to reject the null hypothesis: No significant difference from 5.8 cm.&quot; "],["how-to-perform-a-two-sample-t-test-in-python-and-r-using-the-iris-dataset.html", "24 How to Perform a Two-Sample t-test in Python and R using the iris dataset? 24.1 Explanation 24.2 Python Code 24.3 R Code", " 24 How to Perform a Two-Sample t-test in Python and R using the iris dataset? 24.1 Explanation A Two-Sample t-test is used to determine if there is a significant difference between the means of two independent groups. In this case, we will compare the sepal_length between two species of iris flowers: setosa and versicolor. 24.1.1 Hypotheses: H0 (Null Hypothesis): The mean sepal_length of the two species is equal. H1 (Alternative Hypothesis): The mean sepal_length of the two species is not equal. 24.1.2 Dataset: We will use the iris dataset with columns: sepal_length, sepal_width, petal_length, petal_width, and species, stored in data/iris.csv. 24.2 Python Code import pandas as pd import scipy.stats as stats # Load the iris dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Filter sepal_length for setosa and versicolor species setosa_sepal_length = df[df[&quot;species&quot;] == &quot;setosa&quot;][&quot;sepal_length&quot;] versicolor_sepal_length = df[df[&quot;species&quot;] == &quot;versicolor&quot;][&quot;sepal_length&quot;] # Perform two-sample t-test t_stat, p_value = stats.ttest_ind(setosa_sepal_length, versicolor_sepal_length) # Display results print(f&quot;T-Statistic: {t_stat:.4f}&quot;) print(f&quot;P-Value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: The mean sepal lengths of setosa and versicolor are significantly different.&quot;) else: print(&quot;Fail to reject the null hypothesis: No significant difference between the mean sepal lengths of setosa and versicolor.&quot;) T-Statistic: -10.5210 P-Value: 0.0000 Reject the null hypothesis: The mean sepal lengths of setosa and versicolor are significantly different. 24.3 R Code # Load necessary libraries library(readr) # Load the iris dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Filter sepal_length for setosa and versicolor species setosa_sepal_length &lt;- df[df$species == &quot;setosa&quot;, &quot;sepal_length&quot;] versicolor_sepal_length &lt;- df[df$species == &quot;versicolor&quot;, &quot;sepal_length&quot;] # Perform two-sample t-test t_test_result &lt;- t.test(setosa_sepal_length, versicolor_sepal_length) # Display results print(t_test_result) Welch Two Sample t-test data: setosa_sepal_length and versicolor_sepal_length t = -10.521, df = 86.538, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.1057074 -0.7542926 sample estimates: mean of x mean of y 5.006 5.936 # Interpretation if (t_test_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: The mean sepal lengths of setosa and versicolor are significantly different.&quot;) } else { print(&quot;Fail to reject the null hypothesis: No significant difference between the mean sepal lengths of setosa and versicolor.&quot;) } [1] &quot;Reject the null hypothesis: The mean sepal lengths of setosa and versicolor are significantly different.&quot; "],["qa-how-to-perform-a-paired-t-test-dependent-t-test-in-python-and-r.html", "25 Q&amp;A: How to Perform a Paired t-test (Dependent t-test) in Python and R? 25.1 Explanation 25.2 Python Code 25.3 R Code", " 25 Q&amp;A: How to Perform a Paired t-test (Dependent t-test) in Python and R? 25.1 Explanation A Paired t-test (also known as the Dependent t-test) is used to compare the means of two related groups. It is typically applied when you have two measurements from the same subjects, such as before and after treatment. The test helps determine if the mean difference between two related variables is significantly different from zero. The assumptions of a paired t-test are that the differences between the paired samples are normally distributed. 25.1.1 Null Hypothesis (H‚ÇÄ): The mean difference between the paired samples is equal to zero (no difference). 25.1.2 Alternative Hypothesis (H‚ÇÅ): The mean difference between the paired samples is not equal to zero (a difference exists). 25.2 Python Code import pandas as pd from scipy import stats import warnings # Suppress the specific warning warnings.filterwarnings(&quot;ignore&quot;, category=RuntimeWarning, message=&quot;.*catastrophic cancellation.*&quot;) # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Simulate before and after sepal_length data (for example purposes) # Assuming &#39;sepal_length_before&#39; and &#39;sepal_length_after&#39; are our paired data df[&#39;sepal_length_before&#39;] = df[&#39;sepal_length&#39;] # Example column df[&#39;sepal_length_after&#39;] = df[&#39;sepal_length&#39;] + 0.2 # Example: adding a change # Perform the Paired t-test t_stat, p_value = stats.ttest_rel(df[&#39;sepal_length_before&#39;], df[&#39;sepal_length_after&#39;]) # Print the results print(f&quot;T-statistic: {t_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant difference.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant difference.&quot;) T-statistic: -3.2333821133348364e+16 P-value: 0.0 Reject the null hypothesis: There is a significant difference. 25.3 R Code library(readr) library(stats) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Simulate before and after sepal_length data (for example purposes) # Assuming &#39;sepal_length_before&#39; and &#39;sepal_length_after&#39; are our paired data df$sepal_length_before &lt;- df$sepal_length # Using original data for &quot;before&quot; df$sepal_length_after &lt;- df$sepal_length + rnorm(n = nrow(df), mean = 0.2, sd = 0.1) # Adding some random variation to simulate a change # Perform the Paired t-test test_result &lt;- t.test(df$sepal_length_before, df$sepal_length_after, paired = TRUE) # Print the results print(test_result) Paired t-test data: df$sepal_length_before and df$sepal_length_after t = -24.8, df = 149, p-value &lt; 2.2e-16 alternative hypothesis: true mean difference is not equal to 0 95 percent confidence interval: -0.2166209 -0.1846490 sample estimates: mean difference -0.2006349 # Conclusion based on significance level (alpha = 0.05) if(test_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: There is a significant difference.&quot;) } else { print(&quot;Fail to reject the null hypothesis: There is no significant difference.&quot;) } [1] &quot;Reject the null hypothesis: There is a significant difference.&quot; "],["how-to-perform-one-way-anova-in-python-and-r-using-the-iris-dataset.html", "26 How to Perform One-Way ANOVA in Python and R using the iris dataset? 26.1 Explanation 26.2 Python Code 26.3 R Code 26.4 How to Perform One-Way ANOVA in Python and R? 26.5 R Code", " 26 How to Perform One-Way ANOVA in Python and R using the iris dataset? 26.1 Explanation One-Way ANOVA (Analysis of Variance) is used to determine if there is a significant difference in the means of three or more independent groups. In this case, we will compare the sepal_length across the three species of iris flowers: setosa, versicolor, and virginica. 26.1.1 Hypotheses: H0 (Null Hypothesis): The mean sepal_length of all three species is equal. H1 (Alternative Hypothesis): At least one of the species has a mean sepal_length different from the others. 26.1.2 Dataset: We will use the iris dataset with columns: sepal_length, sepal_width, petal_length, petal_width, and species, stored in data/iris.csv. 26.2 Python Code import pandas as pd import scipy.stats as stats # Load the iris dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Perform One-Way ANOVA f_stat, p_value = stats.f_oneway( df[df[&quot;species&quot;] == &quot;setosa&quot;][&quot;sepal_length&quot;], df[df[&quot;species&quot;] == &quot;versicolor&quot;][&quot;sepal_length&quot;], df[df[&quot;species&quot;] == &quot;virginica&quot;][&quot;sepal_length&quot;] ) # Display results print(f&quot;F-Statistic: {f_stat:.4f}&quot;) print(f&quot;P-Value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: At least one species has a significantly different mean sepal length.&quot;) else: print(&quot;Fail to reject the null hypothesis: No significant difference in mean sepal length among the species.&quot;) F-Statistic: 119.2645 P-Value: 0.0000 Reject the null hypothesis: At least one species has a significantly different mean sepal length. 26.3 R Code # Load necessary libraries library(readr) # Load the iris dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Perform One-Way ANOVA anova_result &lt;- aov(sepal_length ~ species, data = df) # Display results summary(anova_result) Df Sum Sq Mean Sq F value Pr(&gt;F) species 2 63.21 31.606 119.3 &lt;2e-16 *** Residuals 147 38.96 0.265 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Interpretation if (summary(anova_result)[[1]]$`Pr(&gt;F)`[1] &lt; 0.05) { print(&quot;Reject the null hypothesis: At least one species has a significantly different mean sepal length.&quot;) } else { print(&quot;Fail to reject the null hypothesis: No significant difference in mean sepal length among the species.&quot;) } [1] &quot;Reject the null hypothesis: At least one species has a significantly different mean sepal length.&quot; 26.4 How to Perform One-Way ANOVA in Python and R? 26.4.1 Explanation One-Way ANOVA (Analysis of Variance) is used to compare the means of three or more independent groups to see if there is a significant difference between them. It assumes the groups are independent, the data is normally distributed, and the variances are homogeneous across groups. 26.4.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Perform One-Way ANOVA: Comparing sepal_length across different species f_stat, p_value = stats.f_oneway( df[df[&#39;species&#39;] == &#39;setosa&#39;][&#39;sepal_length&#39;], df[df[&#39;species&#39;] == &#39;versicolor&#39;][&#39;sepal_length&#39;], df[df[&#39;species&#39;] == &#39;virginica&#39;][&#39;sepal_length&#39;] ) # Print the results print(f&quot;F-statistic: {f_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant difference between at least one species.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant difference between species.&quot;) F-statistic: 119.26450218450468 P-value: 1.6696691907693826e-31 Reject the null hypothesis: There is a significant difference between at least one species. 26.5 R Code library(readr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Perform One-Way ANOVA: Comparing sepal_length across different species test_result &lt;- aov(sepal_length ~ species, data = df) # Print the summary of the ANOVA test summary_result &lt;- summary(test_result) # Extract the F-statistic and p-value from the summary f_stat &lt;- summary_result[[1]]$`F value`[1] p_value &lt;- summary_result[[1]]$`Pr(&gt;F)`[1] # Print the results cat(&quot;F-statistic:&quot;, f_stat, &quot;\\n&quot;) F-statistic: 119.2645 cat(&quot;P-value:&quot;, p_value, &quot;\\n&quot;) P-value: 1.669669e-31 # Conclusion based on significance level (alpha = 0.05) if (p_value &lt; 0.05) { print(&quot;Reject the null hypothesis: There is a significant difference between at least one species.&quot;) } else { print(&quot;Fail to reject the null hypothesis: There is no significant difference between species.&quot;) } [1] &quot;Reject the null hypothesis: There is a significant difference between at least one species.&quot; "],["how-to-perform-two-way-anova-in-python-and-r.html", "27 How to Perform Two-Way ANOVA in Python and R? 27.1 R Code", " 27 How to Perform Two-Way ANOVA in Python and R? 27.0.1 Explanation Two-Way ANOVA is used when we have two independent variables (factors) and want to understand their individual and combined effects on a dependent variable. It is useful to examine the interaction between two factors, in addition to their main effects. For example, we may want to explore how species and petal length interact to affect the sepal length. 27.0.2 Python Code import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.anova import anova_lm # Load the dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Fit the model model = ols(&#39;sepal_length ~ C(species) + C(petal_length)&#39;, data=df).fit() # Perform Two-Way ANOVA anova_result = anova_lm(model) # Print the results print(anova_result) # Conclusion based on significance level (alpha = 0.05) if anova_result[&#39;PR(&gt;F)&#39;][0] &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant difference between the groups.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant difference between the groups.&quot;) df sum_sq mean_sq F PR(&gt;F) C(species) 2.0 63.212133 31.606067 294.278523 5.378870e-44 C(petal_length) 42.0 27.612422 0.657439 6.121295 2.322488e-14 Residual 106.0 11.384599 0.107402 NaN NaN Reject the null hypothesis: There is a significant difference between the groups. 27.1 R Code library(readr) library(stats) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Fit the model model &lt;- lm(sepal_length ~ species + petal_length, data = df) # Perform Two-Way ANOVA anova_result &lt;- anova(model) # Print the results print(anova_result) Analysis of Variance Table Response: sepal_length Df Sum Sq Mean Sq F value Pr(&gt;F) species 2 63.212 31.6061 276.62 &lt; 2.2e-16 *** petal_length 1 22.275 22.2745 194.95 &lt; 2.2e-16 *** Residuals 146 16.682 0.1143 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Conclusion based on significance level (alpha = 0.05) if (anova_result[&quot;species&quot;, &quot;Pr(&gt;F)&quot;] &lt; 0.05) { print(&quot;Reject the null hypothesis: There is a significant difference between the groups.&quot;) } else { print(&quot;Fail to reject the null hypothesis: There is no significant difference between the groups.&quot;) } [1] &quot;Reject the null hypothesis: There is a significant difference between the groups.&quot; "],["repeated-measures-anova.html", "28 How to Perform a Repeated Measures ANOVA in Python and R? 28.1 Explanation 28.2 Python Code 28.3 R Code", " 28 How to Perform a Repeated Measures ANOVA in Python and R? 28.1 Explanation A Repeated Measures ANOVA is used when you have multiple measurements taken on the same subjects, e.g., before and after treatment on the same individuals. It‚Äôs used to determine if there is a significant difference between the means of three or more groups that are related. 28.2 Python Code import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.stats.anova import anova_lm # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Simulate repeated measures by duplicating rows for a &quot;before&quot; and &quot;after&quot; condition df_long = pd.concat([df.assign(time=&#39;before&#39;), df.assign(time=&#39;after&#39;)], ignore_index=True) # Assign subject IDs (assuming each row in original df is a different subject) df_long[&#39;subject&#39;] = df_long.index % len(df) # Assign a unique subject ID per original row # Fit the model for Repeated Measures ANOVA model = ols(&#39;sepal_length ~ C(time) + C(subject)&#39;, data=df_long).fit() # Perform Repeated Measures ANOVA anova_result = anova_lm(model) # Print the results print(anova_result) # Conclusion based on significance level (alpha = 0.05) if anova_result[&#39;PR(&gt;F)&#39;][0] &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant difference between before and after.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant difference.&quot;) df sum_sq mean_sq F PR(&gt;F) C(time) 1.0 2.608171e-29 2.608171e-29 1.631499e-01 0.686852 C(subject) 149.0 2.043367e+02 1.371387e+00 8.578490e+27 0.000000 Residual 149.0 2.381966e-26 1.598635e-28 NaN NaN Fail to reject the null hypothesis: There is no significant difference. 28.3 R Code # Load necessary libraries library(dplyr) library(car) # For Anova function # Load the dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Simulate repeated measures by duplicating rows for &quot;before&quot; and &quot;after&quot; conditions df_long &lt;- df %&gt;% mutate(time = &quot;before&quot;) %&gt;% bind_rows(df %&gt;% mutate(time = &quot;after&quot;)) # Assign subject IDs (assuming each row in original df is a different subject) df_long$subject &lt;- rep(1:nrow(df), times = 2) # Convert factors df_long$time &lt;- as.factor(df_long$time) df_long$subject &lt;- as.factor(df_long$subject) # Fit the model for Repeated Measures ANOVA model &lt;- aov(sepal_length ~ time + Error(subject), data = df_long) # Perform Repeated Measures ANOVA anova_result &lt;- summary(model) # Print the results print(anova_result) Error: subject Df Sum Sq Mean Sq F value Pr(&gt;F) Residuals 149 204.3 1.371 Error: Within Df Sum Sq Mean Sq F value Pr(&gt;F) time 1 3.000e-31 2.74e-31 0.049 0.825 Residuals 149 8.344e-28 5.60e-30 "],["chi-square-test.html", "29 How to Perform a Chi-Square Test in Python and R? 29.1 Explanation 29.2 Python Code 29.3 R Code", " 29 How to Perform a Chi-Square Test in Python and R? 29.1 Explanation The Chi-Square Test is used to determine if there is a significant association between two categorical variables. It compares the observed frequencies to the expected frequencies under the assumption that no relationship exists between the variables. 29.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Create a contingency table (example) contingency_table = pd.crosstab(df[&#39;species&#39;], df[&#39;sepal_length&#39;]) # Perform the Chi-Square Test chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table) # Print the results print(f&quot;Chi2 Stat: {chi2_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) print(f&quot;Degrees of freedom: {dof}&quot;) print(f&quot;Expected frequencies:\\n{expected}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant association between the variables.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant association between the variables.&quot;) Chi2 Stat: 156.26666666666668 P-value: 6.665987344005466e-09 Degrees of freedom: 68 Expected frequencies: [[0.33333333 1. 0.33333333 1.33333333 0.66666667 1.66666667 2. 3.33333333 3. 1.33333333 0.33333333 2. 2.33333333 2. 2.66666667 2.33333333 1. 2. 2. 1.33333333 3. 2.33333333 1.66666667 0.66666667 2.66666667 1. 1.33333333 0.33333333 0.33333333 1. 0.33333333 0.33333333 0.33333333 1.33333333 0.33333333] [0.33333333 1. 0.33333333 1.33333333 0.66666667 1.66666667 2. 3.33333333 3. 1.33333333 0.33333333 2. 2.33333333 2. 2.66666667 2.33333333 1. 2. 2. 1.33333333 3. 2.33333333 1.66666667 0.66666667 2.66666667 1. 1.33333333 0.33333333 0.33333333 1. 0.33333333 0.33333333 0.33333333 1.33333333 0.33333333] [0.33333333 1. 0.33333333 1.33333333 0.66666667 1.66666667 2. 3.33333333 3. 1.33333333 0.33333333 2. 2.33333333 2. 2.66666667 2.33333333 1. 2. 2. 1.33333333 3. 2.33333333 1.66666667 0.66666667 2.66666667 1. 1.33333333 0.33333333 0.33333333 1. 0.33333333 0.33333333 0.33333333 1.33333333 0.33333333]] Reject the null hypothesis: There is a significant association between the variables. 29.3 R Code library(readr) # Load the dataset df &lt;- read_csv(&#39;data/iris.csv&#39;) # Create a contingency table (example) contingency_table &lt;- table(df$species, df$sepal_length) # Perform the Chi-Square Test chi2_result &lt;- chisq.test(contingency_table) # Print the results print(chi2_result) Pearson&#39;s Chi-squared test data: contingency_table X-squared = 156.27, df = 68, p-value = 6.666e-09 # Conclusion based on significance level (alpha = 0.05) if(chi2_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: There is a significant association between the variables.&quot;) } else { print(&quot;Fail to reject the null hypothesis: There is no significant association between the variables.&quot;) } [1] &quot;Reject the null hypothesis: There is a significant association between the variables.&quot; "],["mann-whitney-u-test.html", "30 How to Perform a Mann-Whitney U Test in Python and R? 30.1 Explanation 30.2 Python Code 30.3 R Code", " 30 How to Perform a Mann-Whitney U Test in Python and R? 30.1 Explanation The Mann-Whitney U Test is a non-parametric test (Non-parametric alternative to t-test), that is used to determine whether there is a significant difference between the distributions of two independent groups. It is the non-parametric alternative to the independent two-sample t-test. 30.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Perform the Mann-Whitney U Test: Comparing sepal_length between two species group_1 = df[df[&#39;species&#39;] == &#39;setosa&#39;][&#39;sepal_length&#39;] group_2 = df[df[&#39;species&#39;] == &#39;versicolor&#39;][&#39;sepal_length&#39;] # Perform the test u_stat, p_value = stats.mannwhitneyu(group_1, group_2) # Print the results print(f&quot;U-statistic: {u_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: There is a significant difference between the groups.&quot;) else: print(&quot;Fail to reject the null hypothesis: There is no significant difference between the groups.&quot;) U-statistic: 168.5 P-value: 8.34582714594069e-14 Reject the null hypothesis: There is a significant difference between the groups. 30.3 R Code library(readr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;, show_col_types = FALSE) # Perform the Mann-Whitney U Test: Comparing sepal_length between two species group_1 &lt;- df$sepal_length[df$species == &quot;setosa&quot;] group_2 &lt;- df$sepal_length[df$species == &quot;versicolor&quot;] # Perform the test test_result &lt;- wilcox.test(group_1, group_2, exact = FALSE) # Print the results print(test_result) Wilcoxon rank sum test with continuity correction data: group_1 and group_2 W = 168.5, p-value = 8.346e-14 alternative hypothesis: true location shift is not equal to 0 # Conclusion based on significance level (alpha = 0.05) if (test_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: There is a significant difference between the groups.&quot;) } else { print(&quot;Fail to reject the null hypothesis: There is no significant difference between the groups.&quot;) } [1] &quot;Reject the null hypothesis: There is a significant difference between the groups.&quot; "],["how-to-perform-a-kruskal-wallis-test-in-python-and-r.html", "31 How to Perform a Kruskal-Wallis Test in Python and R? 31.1 Explanation 31.2 Python Code 31.3 R Code", " 31 How to Perform a Kruskal-Wallis Test in Python and R? (Non-parametric alternative to One-Way ANOVA) 31.1 Explanation The Kruskal-Wallis Test is a non-parametric alternative to One-Way ANOVA used when the assumption of normality is violated. It determines whether there is a statistically significant difference between three or more independent groups. H‚ÇÄ (Null Hypothesis): The distributions of the groups are identical. H‚ÇÅ (Alternative Hypothesis): At least one group has a significantly different distribution. 31.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Perform the Kruskal-Wallis Test: Comparing sepal_length across species group_1 = df[df[&#39;species&#39;] == &#39;setosa&#39;][&#39;sepal_length&#39;] group_2 = df[df[&#39;species&#39;] == &#39;versicolor&#39;][&#39;sepal_length&#39;] group_3 = df[df[&#39;species&#39;] == &#39;virginica&#39;][&#39;sepal_length&#39;] # Perform the test h_stat, p_value = stats.kruskal(group_1, group_2, group_3) # Print the results print(f&quot;H-statistic: {h_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: At least one species has a significantly different sepal length distribution.&quot;) else: print(&quot;Fail to reject the null hypothesis: No significant difference between species.&quot;) H-statistic: 96.93743600064833 P-value: 8.91873433246198e-22 Reject the null hypothesis: At least one species has a significantly different sepal length distribution. 31.3 R Code # Load necessary library library(dplyr) # Load the dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Perform the Kruskal-Wallis Test: Comparing sepal_length across species kruskal_result &lt;- kruskal.test(sepal_length ~ species, data = df) # Print the results print(kruskal_result) Kruskal-Wallis rank sum test data: sepal_length by species Kruskal-Wallis chi-squared = 96.937, df = 2, p-value &lt; 2.2e-16 # Conclusion based on significance level (alpha = 0.05) if (kruskal_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: At least one species has a significantly different sepal length distribution.&quot;) } else { print(&quot;Fail to reject the null hypothesis: No significant difference between species.&quot;) } [1] &quot;Reject the null hypothesis: At least one species has a significantly different sepal length distribution.&quot; "],["how-to-perform-a-friedman-test-in-python-and-r.html", "32 How to Perform a Friedman Test in Python and R? 32.1 Explanation 32.2 Python Code 32.3 R Code", " 32 How to Perform a Friedman Test in Python and R? (Non-parametric alternative to Repeated Measures ANOVA) 32.1 Explanation The Friedman Test is a non-parametric alternative to Repeated Measures ANOVA used when the assumption of normality is violated. It evaluates whether there are significant differences between three or more related groups. H‚ÇÄ (Null Hypothesis): The distributions of the related groups are identical. H‚ÇÅ (Alternative Hypothesis): At least one related group has a significantly different distribution. 32.2 Python Code import pandas as pd import numpy as np from scipy.stats import friedmanchisquare # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Simulate repeated measures data by duplicating rows for different time points df_wide = pd.DataFrame({ &#39;subject&#39;: np.arange(len(df)), # Assign unique subject IDs &#39;time1&#39;: df[&#39;sepal_length&#39;], &#39;time2&#39;: df[&#39;sepal_length&#39;] + 0.2, # Simulating a change &#39;time3&#39;: df[&#39;sepal_length&#39;] + 0.4 }) # Optional: Add small noise (jitter) to avoid division by zero errors # This is useful when there are tied ranks causing infinite Friedman statistic df_wide[[&#39;time1&#39;, &#39;time2&#39;, &#39;time3&#39;]] += np.random.normal(0, 0.001, df_wide[[&#39;time1&#39;, &#39;time2&#39;, &#39;time3&#39;]].shape) # Perform Friedman Test stat, p_value = friedmanchisquare(df_wide[&#39;time1&#39;], df_wide[&#39;time2&#39;], df_wide[&#39;time3&#39;]) # Print the results print(f&quot;Friedman statistic: {stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) if p_value &lt; 0.05: print(&quot;Reject the null hypothesis: At least one time point has a significantly different distribution.&quot;) else: print(&quot;Fail to reject the null hypothesis: No significant difference between time points.&quot;) Friedman statistic: 300.0 P-value: 7.175095973164448e-66 Reject the null hypothesis: At least one time point has a significantly different distribution. 32.3 R Code # Load necessary libraries library(dplyr) library(tidyr) # Load the dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Simulate repeated measures by duplicating rows for different conditions df_long &lt;- bind_rows( df %&gt;% mutate(time = &quot;before&quot;), df %&gt;% mutate(time = &quot;after&quot;), df %&gt;% mutate(time = &quot;followup&quot;) ) # Assign subject IDs (assuming each row in original df is a different subject) df_long$subject &lt;- rep(1:nrow(df), times = 3) # Add a small noise (jitter) to avoid division by zero set.seed(123) # Ensures reproducibility df_long$sepal_length &lt;- df_long$sepal_length + rnorm(nrow(df_long), mean = 0, sd = 0.001) # Reshape data for Friedman test df_wide &lt;- pivot_wider(df_long, names_from = time, values_from = sepal_length) # Perform the Friedman Test friedman_result &lt;- friedman.test(as.matrix(df_wide[, c(&quot;before&quot;, &quot;after&quot;, &quot;followup&quot;)])) # Print the results print(friedman_result) Friedman rank sum test data: as.matrix(df_wide[, c(&quot;before&quot;, &quot;after&quot;, &quot;followup&quot;)]) Friedman chi-squared = 1.2933, df = 2, p-value = 0.5238 # Conclusion based on significance level (alpha = 0.05) if (friedman_result$p.value &lt; 0.05) { print(&quot;Reject the null hypothesis: At least one time point has a significantly different distribution.&quot;) } else { print(&quot;Fail to reject the null hypothesis: No significant difference between time points.&quot;) } [1] &quot;Fail to reject the null hypothesis: No significant difference between time points.&quot; "],["how-to-check-normality-in-python-and-r.html", "33 How to Check Normality in Python and R? 33.1 Explanation 33.2 Python Code 33.3 R Code", " 33 How to Check Normality in Python and R? (Shapiro-Wilk, Kolmogorov-Smirnov tests) 33.1 Explanation Normality tests are used to determine whether a dataset follows a normal (Gaussian) distribution. This is an important assumption for many statistical tests, including t-tests and ANOVA. 33.1.1 Common Normality Tests: Shapiro-Wilk Test: Recommended for small to medium sample sizes (&lt; 50 observations). Kolmogorov-Smirnov (KS) Test: Suitable for larger samples but more sensitive to distribution shape. 33.1.2 Hypotheses: H‚ÇÄ (Null Hypothesis): The data is normally distributed. H‚ÇÅ (Alternative Hypothesis): The data is not normally distributed. 33.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Extract the column to test for normality data = df[&#39;sepal_length&#39;] # Perform Shapiro-Wilk Test shapiro_stat, shapiro_p = stats.shapiro(data) print(f&quot;Shapiro-Wilk Test: W={shapiro_stat}, p-value={shapiro_p}&quot;) # Perform Kolmogorov-Smirnov Test ks_stat, ks_p = stats.kstest(data, &#39;norm&#39;, args=(data.mean(), data.std())) print(f&quot;Kolmogorov-Smirnov Test: KS={ks_stat}, p-value={ks_p}&quot;) # Conclusion alpha = 0.05 if shapiro_p &lt; alpha: print(&quot;Shapiro-Wilk: Reject the null hypothesis (Data is not normally distributed).&quot;) else: print(&quot;Shapiro-Wilk: Fail to reject the null hypothesis (Data is normally distributed).&quot;) if ks_p &lt; alpha: print(&quot;Kolmogorov-Smirnov: Reject the null hypothesis (Data is not normally distributed).&quot;) else: print(&quot;Kolmogorov-Smirnov: Fail to reject the null hypothesis (Data is normally distributed).&quot;) Shapiro-Wilk Test: W=0.9760899543762207, p-value=0.01018026564270258 Kolmogorov-Smirnov Test: KS=0.08865361377316233, p-value=0.1781373784859196 Shapiro-Wilk: Reject the null hypothesis (Data is not normally distributed). Kolmogorov-Smirnov: Fail to reject the null hypothesis (Data is normally distributed). 33.3 R Code # Load necessary library library(readr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Extract the column to test for normality data &lt;- df$sepal_length # Perform Shapiro-Wilk Test shapiro_test &lt;- shapiro.test(data) print(shapiro_test) Shapiro-Wilk normality test data: data W = 0.97609, p-value = 0.01018 # Perform Kolmogorov-Smirnov Test ks_test &lt;- ks.test(data, &quot;pnorm&quot;, mean=mean(data), sd=sd(data)) print(ks_test) Asymptotic one-sample Kolmogorov-Smirnov test data: data D = 0.088654, p-value = 0.1891 alternative hypothesis: two-sided # Conclusion alpha &lt;- 0.05 if (shapiro_test$p.value &lt; alpha) { print(&quot;Shapiro-Wilk: Reject the null hypothesis (Data is not normally distributed).&quot;) } else { print(&quot;Shapiro-Wilk: Fail to reject the null hypothesis (Data is normally distributed).&quot;) } [1] &quot;Shapiro-Wilk: Reject the null hypothesis (Data is not normally distributed).&quot; if (ks_test$p.value &lt; alpha) { print(&quot;Kolmogorov-Smirnov: Reject the null hypothesis (Data is not normally distributed).&quot;) } else { print(&quot;Kolmogorov-Smirnov: Fail to reject the null hypothesis (Data is normally distributed).&quot;) } [1] &quot;Kolmogorov-Smirnov: Fail to reject the null hypothesis (Data is normally distributed).&quot; "],["how-to-perform-a-levenes-test-in-python-and-r-checking-homogeneity-of-variances.html", "34 How to Perform a Levene‚Äôs Test in Python and R? (Checking homogeneity of variances) 34.1 Explanation 34.2 Python Code 34.3 R Code", " 34 How to Perform a Levene‚Äôs Test in Python and R? (Checking homogeneity of variances) 34.1 Explanation Levene‚Äôs test is used to assess the homogeneity of variances across different groups. Homogeneity of variances is a key assumption for parametric tests like ANOVA. 34.1.1 Hypotheses: H‚ÇÄ (Null Hypothesis): The variances of the groups are equal (homoscedasticity). H‚ÇÅ (Alternative Hypothesis): At least one group has a significantly different variance (heteroscedasticity). 34.2 Python Code import pandas as pd from scipy import stats # Load the dataset df = pd.read_csv(&#39;data/iris.csv&#39;) # Extract groups based on species group_1 = df[df[&#39;species&#39;] == &#39;setosa&#39;][&#39;sepal_length&#39;] group_2 = df[df[&#39;species&#39;] == &#39;versicolor&#39;][&#39;sepal_length&#39;] group_3 = df[df[&#39;species&#39;] == &#39;virginica&#39;][&#39;sepal_length&#39;] # Perform Levene‚Äôs Test levene_stat, p_value = stats.levene(group_1, group_2, group_3) # Print the results print(f&quot;Levene‚Äôs Test Statistic: {levene_stat}&quot;) print(f&quot;P-value: {p_value}&quot;) # Conclusion based on significance level (alpha = 0.05) alpha = 0.05 if p_value &lt; alpha: print(&quot;Reject the null hypothesis: Variances are significantly different.&quot;) else: print(&quot;Fail to reject the null hypothesis: Variances are equal.&quot;) Levene‚Äôs Test Statistic: 6.35272002048269 P-value: 0.0022585277836218586 Reject the null hypothesis: Variances are significantly different. 34.3 R Code # Load necessary libraries library(car) library(readr) # Load the dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Perform Levene‚Äôs Test levene_test &lt;- leveneTest(sepal_length ~ species, data = df) # Print the results print(levene_test) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 2 6.3527 0.002259 ** 147 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Conclusion alpha &lt;- 0.05 if (levene_test$&quot;Pr(&gt;F)&quot;[1] &lt; alpha) { print(&quot;Reject the null hypothesis: Variances are significantly different.&quot;) } else { print(&quot;Fail to reject the null hypothesis: Variances are equal.&quot;) } [1] &quot;Reject the null hypothesis: Variances are significantly different.&quot; "],["how-to-compute-the-mean-median-and-mode-of-a-dataset.html", "35 How to compute the mean, median, and mode of a dataset? 35.1 Explanation 35.2 Python Code 35.3 R Code", " 35 How to compute the mean, median, and mode of a dataset? 35.1 Explanation Mean: The average of all values in the dataset. Median: The middle value when the data is sorted. Mode: The value that appears most frequently in the dataset. 35.2 Python Code import pandas as pd # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Compute mean, median, and mode mean_values = df.drop(columns=[&quot;species&quot;]).mean() median_values = df.drop(columns=[&quot;species&quot;]).median() mode_values = df.drop(columns=[&quot;species&quot;]).mode().iloc[0] # Display results print(&quot;Mean:\\n&quot;) print(mean_values) print(&quot;\\nMedian:\\n&quot;) print(median_values) print(&quot;\\nMode:\\n&quot;) print(mode_values) Mean: sepal_length 5.843333 sepal_width 3.057333 petal_length 3.758000 petal_width 1.199333 dtype: float64 Median: sepal_length 5.80 sepal_width 3.00 petal_length 4.35 petal_width 1.30 dtype: float64 Mode: sepal_length 5.0 sepal_width 3.0 petal_length 1.4 petal_width 0.2 Name: 0, dtype: float64 35.3 R Code # Load necessary libraries library(tidyverse) # Load dataset df &lt;- read_csv(&quot;data/iris.csv&quot;) # Compute mean, median, and mode mean_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), mean)) median_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), median)) mode_values &lt;- df %&gt;% select(-species) %&gt;% summarise(across(everything(), ~ names(sort(table(.), decreasing = TRUE))[1])) # Display results print(&quot;Mean:&quot;) [1] &quot;Mean:&quot; print(mean_values) # A tibble: 1 √ó 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5.84 3.06 3.76 1.20 print(&quot;Median:&quot;) [1] &quot;Median:&quot; print(median_values) # A tibble: 1 √ó 4 sepal_length sepal_width petal_length petal_width &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 5.8 3 4.35 1.3 print(&quot;Mode:&quot;) [1] &quot;Mode:&quot; print(mode_values) # A tibble: 1 √ó 4 sepal_length sepal_width petal_length petal_width &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 5 3 1.4 0.2 "],["how-to-perform-a-pearson-correlation-test-in-python-and-r.html", "36 How to Perform a Pearson Correlation Test in Python and R? 36.1 Explanation 36.2 Python Code 36.3 R Code", " 36 How to Perform a Pearson Correlation Test in Python and R? 36.1 Explanation The Pearson Correlation Test is used to determine the linear relationship between two continuous variables. It measures the strength and direction of the relationship, with a correlation coefficient (\\(r\\)) ranging from -1 to 1: - \\(r = 1\\) indicates a perfect positive linear relationship. - \\(r = -1\\) indicates a perfect negative linear relationship. - \\(r = 0\\) indicates no linear relationship. The null hypothesis (\\(H_0\\)) assumes that there is no linear correlation between the two variables, while the alternative hypothesis (\\(H_a\\)) states that there is a linear correlation. If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that there is a significant linear relationship between the two variables. 36.2 Python Code import pandas as pd from scipy.stats import pearsonr # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select two variables for correlation test x = df[&#39;sepal_length&#39;] y = df[&#39;sepal_width&#39;] # Perform Pearson Correlation Test corr_coefficient, p_value = pearsonr(x, y) # Display results print(f&quot;Pearson Correlation Coefficient: {corr_coefficient:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) # Interpretation if p_value &lt; 0.05: print(&quot;Reject H0: There is a significant linear correlation between sepal length and sepal width.&quot;) else: print(&quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot;) Pearson Correlation Coefficient: -0.1176 P-value: 0.1519 Fail to reject H0: No significant linear correlation between sepal length and sepal width. 36.3 R Code # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Select two variables for correlation test x &lt;- df$sepal_length y &lt;- df$sepal_width # Perform Pearson Correlation Test cor_result &lt;- cor.test(x, y) # Display results print(cor_result) Pearson&#39;s product-moment correlation data: x and y t = -1.4403, df = 148, p-value = 0.1519 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.27269325 0.04351158 sample estimates: cor -0.1175698 # Interpretation if (cor_result$p.value &lt; 0.05) { print(&quot;Reject H0: There is a significant linear correlation between sepal length and sepal width.&quot;) } else { print(&quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot;) } [1] &quot;Fail to reject H0: No significant linear correlation between sepal length and sepal width.&quot; "],["how-to-perform-a-simple-linear-regression-in-python-and-r.html", "37 How to Perform a Simple Linear Regression in Python and R? 37.1 Explanation 37.2 Python Code 37.3 R Code", " 37 How to Perform a Simple Linear Regression in Python and R? 37.1 Explanation Simple Linear Regression is used to model the relationship between a dependent variable (\\(Y\\)) and an independent variable (\\(X\\)) by fitting a linear equation to observed data. The goal is to find the best-fitting line, represented by the equation: \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\] Where: - \\(Y\\) is the dependent variable, - \\(X\\) is the independent variable, - \\(\\beta_0\\) is the intercept, - \\(\\beta_1\\) is the slope, - \\(\\epsilon\\) is the error term. The null hypothesis (\\(H_0\\)) assumes that the slope of the regression line is zero, meaning there is no linear relationship between \\(X\\) and \\(Y\\). The alternative hypothesis (\\(H_a\\)) states that the slope is non-zero, indicating a significant relationship. If the p-value is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that there is a significant relationship between \\(X\\) and \\(Y\\). 37.2 Python Code Simple Linear Regression import pandas as pd import statsmodels.api as sm # Load dataset df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent and dependent variables X = df[&#39;sepal_length&#39;] y = df[&#39;sepal_width&#39;] # Add a constant (intercept) to the independent variable X = sm.add_constant(X) # Perform linear regression model = sm.OLS(y, X).fit() # Display results print(model.summary()) # Interpretation if model.pvalues[1] &lt; 0.05: print(&quot;Reject H0: There is a significant linear relationship between sepal length and sepal width.&quot;) else: print(&quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot;) OLS Regression Results ============================================================================== Dep. Variable: sepal_width R-squared: 0.014 Model: OLS Adj. R-squared: 0.007 Method: Least Squares F-statistic: 2.074 Date: Wed, 19 Mar 2025 Prob (F-statistic): 0.152 Time: 23:05:02 Log-Likelihood: -86.732 No. Observations: 150 AIC: 177.5 Df Residuals: 148 BIC: 183.5 Df Model: 1 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const 3.4189 0.254 13.484 0.000 2.918 3.920 sepal_length -0.0619 0.043 -1.440 0.152 -0.147 0.023 ============================================================================== Omnibus: 2.474 Durbin-Watson: 1.263 Prob(Omnibus): 0.290 Jarque-Bera (JB): 1.994 Skew: 0.243 Prob(JB): 0.369 Kurtosis: 3.288 Cond. No. 43.4 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Fail to reject H0: No significant linear relationship between sepal length and sepal width. 37.3 R Code Simple Linear Regression # Load dataset df &lt;- read.csv(&quot;data/iris.csv&quot;) # Select independent and dependent variables X &lt;- df$sepal_length y &lt;- df$sepal_width # Perform linear regression model &lt;- lm(y ~ X) # Display results summary(model) Call: lm(formula = y ~ X) Residuals: Min 1Q Median 3Q Max -1.1095 -0.2454 -0.0167 0.2763 1.3338 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.41895 0.25356 13.48 &lt;2e-16 *** X -0.06188 0.04297 -1.44 0.152 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4343 on 148 degrees of freedom Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 # Interpretation if (summary(model)$coefficients[2,4] &lt; 0.05) { print(&quot;Reject H0: There is a significant linear relationship between sepal length and sepal width.&quot;) } else { print(&quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot;) } [1] &quot;Fail to reject H0: No significant linear relationship between sepal length and sepal width.&quot; "],["how-to-perform-multiple-linear-regression-in-python-and-r.html", "38 How to Perform Multiple Linear Regression in Python and R? 38.1 Explanation 38.2 Python Code 38.3 R Code", " 38 How to Perform Multiple Linear Regression in Python and R? 38.1 Explanation Multiple Linear Regression is an extension of simple linear regression that models the relationship between a dependent variable (\\(Y\\)) and two or more independent variables (\\(X_1, X_2, \\dots, X_n\\)). The equation for multiple linear regression is: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon \\] Where: - \\(Y\\) is the dependent variable, - \\(X_1, X_2, \\dots, X_n\\) are the independent variables, - \\(\\beta_0\\) is the intercept, - \\(\\beta_1, \\beta_2, \\dots, \\beta_n\\) are the coefficients (slopes) for the predictors, - \\(\\epsilon\\) is the error term. The null hypothesis (\\(H_0\\)) assumes that all regression coefficients are zero, meaning no relationship exists between the predictors and the dependent variable. The alternative hypothesis (\\(H_a\\)) suggests that at least one coefficient is non-zero, indicating a significant relationship. If the p-value for a coefficient is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis for that predictor and conclude that it significantly affects \\(Y\\). 38.2 Python Code import pandas as pd import statsmodels.api as sm # Load dataset (you can use any dataset here) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Predictors # Add a constant (intercept) to the model X = sm.add_constant(X) # Dependent variable (response) y = df[&#39;sepal_length&#39;] # Example of using sepal length as the dependent variable # Fit the model model = sm.OLS(y, X).fit() # Display the results print(model.summary()) OLS Regression Results ============================================================================== Dep. Variable: sepal_length R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 9.587e+29 Date: Wed, 19 Mar 2025 Prob (F-statistic): 0.00 Time: 23:05:02 Log-Likelihood: 4724.3 No. Observations: 150 AIC: -9439. Df Residuals: 145 BIC: -9424. Df Model: 4 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const -9.104e-15 4.83e-15 -1.887 0.061 -1.86e-14 4.34e-16 sepal_length 1.0000 1.36e-15 7.36e+14 0.000 1.000 1.000 sepal_width 1.11e-15 1.41e-15 0.790 0.431 -1.67e-15 3.89e-15 petal_length 2.776e-16 1.34e-15 0.207 0.836 -2.37e-15 2.92e-15 petal_width -1.554e-15 2.23e-15 -0.698 0.486 -5.95e-15 2.84e-15 ============================================================================== Omnibus: 5.539 Durbin-Watson: 0.030 Prob(Omnibus): 0.063 Jarque-Bera (JB): 3.049 Skew: 0.099 Prob(JB): 0.218 Kurtosis: 2.330 Cond. No. 91.9 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 38.3 R Code Multiple Linear Regression # Check if &#39;caret&#39; is installed, if not, install it if (!require(caret)) { # Set CRAN mirror options(repos = c(CRAN = &quot;https://cran.rstudio.com/&quot;)) install.packages(&quot;caret&quot;) library(caret) } # Load dataset (use any dataset available) df &lt;- read.csv(&quot;data/iris.csv&quot;) # Create training and test sets set.seed(123) # For reproducibility trainIndex &lt;- createDataPartition(df$sepal_length, p = 0.7, list = FALSE) trainData &lt;- df[trainIndex, ] testData &lt;- df[-trainIndex, ] # Train the linear regression model model &lt;- train(sepal_length ~ sepal_width + petal_length + petal_width, data = trainData, method = &quot;lm&quot;) # Display the model details print(model) Linear Regression 107 samples 3 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 107, 107, 107, 107, 107, 107, ... Resampling results: RMSE Rsquared MAE 0.3188897 0.8743628 0.26081 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # Predict on the test set predictions &lt;- predict(model, testData) # Display predictions and actual values predictions 1 2 7 15 18 23 25 27 5.024336 4.670085 4.878372 5.219239 4.949222 4.776489 5.351858 4.962606 30 31 33 36 42 49 52 56 4.971134 4.900284 5.604227 4.652436 4.019344 5.245711 6.305208 6.172036 59 67 68 74 75 77 82 85 6.322560 6.163508 6.007830 6.406499 6.083537 6.335945 5.476581 6.163508 87 88 91 93 99 100 105 108 6.393707 5.738110 6.025775 5.707077 4.914596 5.853338 6.673476 7.301455 111 116 124 126 127 128 133 142 6.407684 6.341690 6.044312 7.274983 6.035488 6.256863 6.372426 6.111491 144 146 149 6.819737 6.120315 6.563065 testData$sepal_length [1] 5.1 4.9 4.6 5.8 5.1 4.6 4.8 5.0 4.7 4.8 5.2 5.0 4.5 5.3 6.4 5.7 6.6 5.6 5.8 [20] 6.1 6.4 6.8 5.5 5.4 6.7 6.3 5.5 5.8 5.1 5.7 6.5 7.3 6.5 6.4 6.3 7.2 6.2 6.1 [39] 6.4 6.9 6.8 6.7 6.2 # Evaluate the model using RMSE (Root Mean Squared Error) rmse &lt;- sqrt(mean((predictions - testData$sepal_length)^2)) cat(&quot;RMSE: &quot;, rmse) RMSE: 0.3445695 "],["how-to-perform-logistic-regression-in-python-and-r.html", "39 How to Perform Logistic Regression in Python and R? 39.1 Explanation 39.2 Python Code 39.3 R Code", " 39 How to Perform Logistic Regression in Python and R? 39.1 Explanation Logistic Regression is a statistical method used for binary classification. It models the relationship between a dependent variable (binary outcome, e.g., 0 or 1) and one or more independent variables. The logistic regression equation is: \\[ \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n \\] Where: - \\(p\\) is the probability of the dependent event occurring (i.e., the probability that the output is 1), - \\(X_1, X_2, \\dots, X_n\\) are the independent variables (predictors), - \\(\\beta_0\\) is the intercept, - \\(\\beta_1, \\beta_2, \\dots, \\beta_n\\) are the coefficients (slopes) for the predictors. The model estimates the odds of the event occurring by taking the log of the odds ratio. The predicted probabilities are obtained using the logistic function, which is: \\[ p = \\frac{1}{1 + e^{-z}} \\] Where \\(z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\\). If the p-value of a coefficient is small (typically \\(p &lt; 0.05\\)), we reject the null hypothesis and conclude that the predictor significantly influences the outcome. 39.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any binary classification dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Predictors # Create a binary dependent variable (response) df[&#39;is_setosa&#39;] = (df[&#39;species&#39;] == &#39;setosa&#39;).astype(int) # Convert &#39;setosa&#39; to 1, others to 0 y = df[&#39;is_setosa&#39;] # Target variable # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the logistic regression model model = LogisticRegression() model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[26 0] [ 0 19]] 39.3 R Code # Import necessary libraries library(readr) # For reading data library(caret) # For model training and evaluation library(dplyr) # For data manipulation # Load dataset with show_col_types = FALSE df &lt;- read_csv(&quot;data/iris.csv&quot;, show_col_types = FALSE) # Select independent variables (predictors) X &lt;- df %&gt;% select(sepal_length, sepal_width, petal_length, petal_width) # Create a binary dependent variable (response) df$is_setosa &lt;- ifelse(df$species == &#39;setosa&#39;, 1, 0) # Convert &#39;setosa&#39; to 1, others to 0 y &lt;- df$is_setosa # Target variable # Split data into training and testing sets (70% training, 30% testing) set.seed(42) # For reproducibility trainIndex &lt;- createDataPartition(y, p = 0.7, list = FALSE) X_train &lt;- X[trainIndex, ] y_train &lt;- y[trainIndex] X_test &lt;- X[-trainIndex, ] y_test &lt;- y[-trainIndex] # Fit the logistic regression model model &lt;- glm(is_setosa ~ sepal_length + sepal_width + petal_length + petal_width, data = df[trainIndex, ], family = binomial()) # Make predictions on the test set y_pred_prob &lt;- predict(model, X_test, type = &quot;response&quot;) y_pred &lt;- ifelse(y_pred_prob &gt; 0.5, 1, 0) # Convert probabilities to binary outcomes # Evaluate the model accuracy &lt;- mean(y_pred == y_test) conf_matrix &lt;- table(Predicted = y_pred, Actual = y_test) # Print results cat(&quot;Accuracy:&quot;, accuracy, &quot;\\n&quot;) Accuracy: 1 print(conf_matrix) Actual Predicted 0 1 0 33 0 1 0 12 "],["how-to-perform-support-vector-machine-svm-classification-in-python-and-r.html", "40 How to Perform Support Vector Machine (SVM) Classification in Python and R? 40.1 Explanation 40.2 Python Code 40.3 R Code", " 40 How to Perform Support Vector Machine (SVM) Classification in Python and R? 40.1 Explanation Support Vector Machine (SVM) is a powerful supervised learning algorithm that can be used for both classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes in a high-dimensional space. Linear SVM: Finds a straight line or hyperplane that divides the classes. Non-linear SVM: Uses kernel functions (like Radial Basis Function (RBF)) to transform the data into higher dimensions to make it linearly separable. The main objective of SVM is to maximize the margin between the two classes. The margin is the distance between the hyperplane and the closest data points from either class, known as support vectors. The SVM classifier works well for both linear and non-linear classification problems. 40.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset (you can use any dataset) df = pd.read_csv(&quot;data/iris.csv&quot;) # Select independent variables (predictors) X = df[[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]] # Features # Select target variable (species) y = df[&#39;species&#39;] # Target # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the SVM classifier model = SVC(kernel=&#39;linear&#39;, random_state=42) model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Confusion Matrix:\\n&quot;, confusion_matrix(y_test, y_pred)) Accuracy: 1.0 Confusion Matrix: [[19 0 0] [ 0 13 0] [ 0 0 13]] 40.3 R Code "],["summary.html", "Summary", " Summary In this beginner guide, we have laid the foundation for your data science journey. We started by introducing key concepts like datasets, data manipulation, basic statistical analysis, and data visualization. We also walked through hands-on exercises using Python and R, which helped solidify these concepts with practical examples. Through over 30 Q&amp;A sections, you‚Äôve learned how to: Work with datasets and perform basic data cleaning and exploration. Visualize data using popular plotting libraries. Conduct statistical analysis and hypothesis testing. Train and evaluate machine learning models, starting with simple models like logistic regression and moving to more complex models like random forests. This guide has provided you with the essential tools and knowledge to get started in data science. By now, you should be comfortable working with data, understanding basic models, and using Python and R for data-related tasks. "],["whats-next-transition-to-intermediate-version.html", "What‚Äôs Next: Transition to Intermediate Version", " What‚Äôs Next: Transition to Intermediate Version Congratulations on completing the beginner guide! You‚Äôve now acquired the foundational skills necessary to advance your data science journey. If you feel ready to take your learning further, the Intermediate Version is the next step. In the Intermediate Version, we will dive deeper into more advanced topics, including: Data Cleaning and Preprocessing: Handling missing values, feature scaling, and encoding categorical variables. Advanced Data Visualization: Mastering visualizations with libraries like Seaborn, Matplotlib, and ggplot2. Machine Learning: Exploring advanced algorithms like Decision Trees, Support Vector Machines (SVM), and Neural Networks. Model Evaluation and Optimization: Fine-tuning models, cross-validation, and understanding metrics beyond accuracy. Real-world Projects: Applying your knowledge to real-world datasets and solving complex problems. By moving to the intermediate version, you‚Äôll develop the expertise needed to tackle larger datasets, handle more sophisticated analyses, and begin building real-world applications. Are you ready to take the next step? We can‚Äôt wait to continue this exciting journey with you! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
