{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (PART) MACHINE LEARNING {-}\n",
    "\n",
    "# How to Perform Logistic Regression in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "**Logistic Regression** is a statistical method used for binary classification. It models the relationship between a dependent variable (binary outcome, e.g., 0 or 1) and one or more independent variables. The logistic regression equation is:\n",
    "\n",
    "\\[\n",
    "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( p \\) is the probability of the dependent event occurring (i.e., the probability that the output is 1),\n",
    "- \\( X_1, X_2, \\dots, X_n \\) are the independent variables (predictors),\n",
    "- \\( \\beta_0 \\) is the intercept,\n",
    "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients (slopes) for the predictors.\n",
    "\n",
    "The model estimates the odds of the event occurring by taking the log of the odds ratio. The predicted probabilities are obtained using the logistic function, which is:\n",
    "\n",
    "\\[\n",
    "p = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "Where \\( z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n \\).\n",
    "\n",
    "If the p-value of a coefficient is small (typically \\( p < 0.05 \\)), we reject the null hypothesis and conclude that the predictor significantly influences the outcome.\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[26  0]\n",
      " [ 0 19]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load dataset (you can use any binary classification dataset)\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select independent variables (predictors)\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]  # Predictors\n",
    "\n",
    "# Create a binary dependent variable (response)\n",
    "df['is_setosa'] = (df['species'] == 'setosa').astype(int)  # Convert 'setosa' to 1, others to 0\n",
    "y = df['is_setosa']  # Target variable\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r, eval=FALSE}\n",
    "# Load required library\n",
    "if (!require(caret)) {\n",
    "  install.packages(\"caret\")\n",
    "  library(caret)\n",
    "}\n",
    "\n",
    "# Load dataset (use any binary classification dataset)\n",
    "df <- read.csv(\"data/iris.csv\")\n",
    "\n",
    "# Create a binary dependent variable (response)\n",
    "df$is_setosa <- ifelse(df$species == 'setosa', 1, 0)  # Convert 'setosa' to 1, others to 0\n",
    "\n",
    "# Create training and test sets\n",
    "set.seed(123)  # For reproducibility\n",
    "trainIndex <- createDataPartition(df$is_setosa, p = 0.7, list = FALSE)\n",
    "trainData <- df[trainIndex, ]\n",
    "testData <- df[-trainIndex, ]\n",
    "\n",
    "# Train the logistic regression model\n",
    "model <- train(is_setosa ~ sepal_length + sepal_width + petal_length + petal_width,\n",
    "               data = trainData,\n",
    "               method = \"glm\",\n",
    "               family = \"binomial\",\n",
    "               trControl = trainControl(method = \"cv\", number = 10))\n",
    "\n",
    "# Display the model details\n",
    "print(model)\n",
    "\n",
    "# Predict the probabilities using the logistic regression model\n",
    "pred_probs <- predict(model, testData, type = \"prob\")[, 2]  # Get the probability of class 1\n",
    "\n",
    "# Convert probabilities to class predictions (0 or 1)\n",
    "pred_class <- ifelse(pred_probs > 0.5, 1, 0)\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "conf_matrix <- confusionMatrix(factor(pred_class), factor(testData$is_setosa))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(conf_matrix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Perform Decision Tree Classification in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "**Decision Trees** are a non-linear model used for both classification and regression. They split the dataset into subsets based on the most significant features, creating a tree-like structure where each internal node represents a feature or attribute, and each leaf node represents a decision or class label.\n",
    "\n",
    "The decision tree algorithm works by selecting the feature that best splits the data at each node based on certain criteria (e.g., Gini impurity, entropy for classification, and variance reduction for regression). \n",
    "\n",
    "- **Classification Trees**: Used when the target variable is categorical. For example, in a binary classification problem, the decision tree can predict class 0 or class 1 based on the input features.\n",
    "- **Regression Trees**: Used when the target variable is continuous.\n",
    "\n",
    "The tree is built recursively by selecting the best splits at each node and stopping when a stopping criterion (e.g., maximum depth, minimum samples per leaf) is met.\n",
    "\n",
    "## Python Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load dataset (you can use any dataset)\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select independent variables (predictors)\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]  # Features\n",
    "\n",
    "# Select target variable (species)\n",
    "y = df['species']  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the decision tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r, eval=FALSE}\n",
    "# Load required library\n",
    "library(caret)\n",
    "\n",
    "# Load dataset (use any dataset)\n",
    "df <- read.csv(\"data/iris.csv\")\n",
    "\n",
    "# Create training and test sets\n",
    "set.seed(123)  # For reproducibility\n",
    "trainIndex <- createDataPartition(df$species, p = 0.7, list = FALSE)\n",
    "trainData <- df[trainIndex, ]\n",
    "testData <- df[-trainIndex, ]\n",
    "\n",
    "# Train the model using decision tree classifier\n",
    "model <- train(species ~ sepal_length + sepal_width + petal_length + petal_width,\n",
    "               data = trainData,\n",
    "               method = \"rpart\",\n",
    "               trControl = trainControl(method = \"cv\", number = 10))\n",
    "\n",
    "# Display the model details\n",
    "print(model)\n",
    "\n",
    "# Predict using the decision tree model\n",
    "pred <- predict(model, testData)\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "confusionMatrix(pred, testData$species)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Perform Random Forest Classification in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "**Random Forest** is an ensemble learning method that combines multiple decision trees to improve classification accuracy. Instead of relying on a single decision tree, random forest aggregates the predictions of many trees, reducing overfitting and improving generalization.\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)**: Random Forest uses bagging, which means training multiple models (decision trees) on random subsets of the data. Each tree is trained on a different random sample, and the final prediction is made by averaging (for regression) or majority voting (for classification) of all the trees' predictions.\n",
    "- **Random Feature Selection**: At each split in the decision tree, a random subset of features is selected, ensuring that trees are diverse and reducing the correlation between them.\n",
    "\n",
    "The main advantages of random forest are its robustness, the ability to handle a large number of features, and its capacity to deal with overfitting.\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load dataset (you can use any dataset)\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select independent variables (predictors)\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]  # Features\n",
    "\n",
    "# Select target variable (species)\n",
    "y = df['species']  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the random forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r, eval=FALSE}\n",
    "# Load required library\n",
    "library(caret)\n",
    "\n",
    "# Load dataset (use any dataset)\n",
    "df <- read.csv(\"data/iris.csv\")\n",
    "\n",
    "# Create training and test sets\n",
    "set.seed(123)  # For reproducibility\n",
    "trainIndex <- createDataPartition(df$species, p = 0.7, list = FALSE)\n",
    "trainData <- df[trainIndex, ]\n",
    "testData <- df[-trainIndex, ]\n",
    "\n",
    "# Train the model using Random Forest\n",
    "model <- train(species ~ sepal_length + sepal_width + petal_length + petal_width,\n",
    "               data = trainData,\n",
    "               method = \"rf\",\n",
    "               trControl = trainControl(method = \"cv\", number = 10))\n",
    "\n",
    "# Display the model details\n",
    "print(model)\n",
    "\n",
    "# Predict using the random forest model\n",
    "pred <- predict(model, testData)\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "confusionMatrix(pred, testData$species)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Perform Support Vector Machine (SVM) Classification in Python and R?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "**Support Vector Machine (SVM)** is a powerful supervised learning algorithm that can be used for both classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes in a high-dimensional space.\n",
    "\n",
    "- **Linear SVM**: Finds a straight line or hyperplane that divides the classes.\n",
    "- **Non-linear SVM**: Uses kernel functions (like Radial Basis Function (RBF)) to transform the data into higher dimensions to make it linearly separable.\n",
    "\n",
    "The main objective of SVM is to maximize the margin between the two classes. The margin is the distance between the hyperplane and the closest data points from either class, known as support vectors.\n",
    "\n",
    "The SVM classifier works well for both linear and non-linear classification problems.\n",
    "\n",
    "## Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load dataset (you can use any dataset)\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Select independent variables (predictors)\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]  # Features\n",
    "\n",
    "# Select target variable (species)\n",
    "y = df['species']  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the SVM classifier\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Code\n",
    "\n",
    "```{r, eval=FALSE}\n",
    "# Load required library\n",
    "library(caret)\n",
    "\n",
    "# Load dataset (use any dataset)\n",
    "df <- read.csv(\"data/iris.csv\")\n",
    "\n",
    "# Create training and test sets\n",
    "set.seed(123)  # For reproducibility\n",
    "trainIndex <- createDataPartition(df$species, p = 0.7, list = FALSE)\n",
    "trainData <- df[trainIndex, ]\n",
    "testData <- df[-trainIndex, ]\n",
    "\n",
    "# Train the model using SVM with a linear kernel\n",
    "model <- train(species ~ sepal_length + sepal_width + petal_length + petal_width,\n",
    "               data = trainData,\n",
    "               method = \"svmLinear\",\n",
    "               trControl = trainControl(method = \"cv\", number = 10))\n",
    "\n",
    "# Display the model details\n",
    "print(model)\n",
    "\n",
    "# Predict using the SVM model\n",
    "pred <- predict(model, testData)\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "confusionMatrix(pred, testData$species)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
